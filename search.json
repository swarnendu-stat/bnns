[{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 bnns authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Benchmarking bnns","text":"article demonstrates performance bnns package three datasets mlbench package: Regression: mlbench.friedman1 dataset Binary Classification: mlbench.spirals dataset Multi-class Classification: mlbench.waveform dataset dataset, : Prepare data training testing. Build Bayesian Neural Network using bnns package. Evaluate model’s predictive performance. compare, show performance randomforest algorithm ranger package default settings.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"dataset-description","dir":"Articles","previous_headings":"Regression: Friedman1 Dataset","what":"Dataset Description","title":"Benchmarking bnns","text":"dataset generated mlbench.friedman1 regression problem Friedman 1 described Friedman (1991) Breiman (1996). Inputs 10 independent variables uniformly distributed interval [0,1], 5 10 actually used. Outputs created according formula y=10sin(πx1x2)+20(x3−0.5)2+10x4+5x5+ϵ y = 10 \\sin(\\pi x_1 x_2) + 20 (x_3 - 0.5)^2 + 10 x_4 + 5 x_5 + \\epsilon ϵ∼N(0,1).\\epsilon \\sim N(0, 1).","code":"# Generating the Friedman1 dataset friedman1_data <- mlbench.friedman1(n = 100, sd = 1)  # Splitting the data into training (80%) and testing (20%) sets friedman1_split <- initial_split(   cbind.data.frame(y = friedman1_data$y, friedman1_data$x),   prop = 0.8 ) friedman1_train <- training(friedman1_split)  # Training data friedman1_test <- testing(friedman1_split)   # Testing data"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-training","dir":"Articles","previous_headings":"Regression: Friedman1 Dataset","what":"Model Training","title":"Benchmarking bnns","text":"","code":"# Training a Bayesian Neural Network with a single hidden layer and 4 nodes friedman1_bnn <- bnns(y ~ -1 + .,   data = friedman1_train, L = 1, nodes = 4, act_fn = 3,   out_act_fn = 1, iter = 1e3, warmup = 2e2 ) #> Trying to compile a simple C file #> Running /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c #> using C compiler: ‘gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0’ #> gcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/home/runner/work/_temp/Library/Rcpp/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/unsupported\"  -I\"/home/runner/work/_temp/Library/BH/include\" -I\"/home/runner/work/_temp/Library/StanHeaders/include/src/\"  -I\"/home/runner/work/_temp/Library/StanHeaders/include/\"  -I\"/home/runner/work/_temp/Library/RcppParallel/include/\"  -I\"/home/runner/work/_temp/Library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o #> In file included from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Core:19, #>                  from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Dense:1, #>                  from /home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22, #>                  from <command-line>: #> /home/runner/work/_temp/Library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory #>   679 | #include <cmath> #>       |          ^~~~~~~ #> compilation terminated. #> make: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1 #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.000145 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.45 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 0.000142 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.42 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 1.337 seconds (Warm-up) #> Chain 2:                4.716 seconds (Sampling) #> Chain 2:                6.053 seconds (Total) #> Chain 2:  #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 1.241 seconds (Warm-up) #> Chain 1:                5.715 seconds (Sampling) #> Chain 1:                6.956 seconds (Total) #> Chain 1:"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-evaluation","dir":"Articles","previous_headings":"Regression: Friedman1 Dataset","what":"Model Evaluation","title":"Benchmarking bnns","text":"","code":"# Making predictions on the test set and evaluating model performance friedman1_bnn_pred <- predict(friedman1_bnn, friedman1_test) measure_cont(friedman1_test$y, friedman1_bnn_pred)  # Measures like RMSE, MAE #> $rmse #> [1] 2.328514 #>  #> $mae #> [1] 1.894961"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-comparison","dir":"Articles","previous_headings":"Regression: Friedman1 Dataset","what":"Model Comparison","title":"Benchmarking bnns","text":"","code":"# Training a random forest model for comparison friedman1_rf <- ranger(   y ~ -1 + .,   data = friedman1_train |> `colnames<-`(c(\"y\", paste0(\"x\", 1:10))) )  # Making predictions with random forest and evaluating performance friedman1_rf_pred <- predict(   friedman1_rf,   friedman1_test |> `colnames<-`(c(\"y\", paste0(\"x\", 1:10))) ) measure_cont(friedman1_test$y, friedman1_rf_pred$predictions) #> $rmse #> [1] 3.099387 #>  #> $mae #> [1] 2.517299"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"dataset-description-1","dir":"Articles","previous_headings":"Binary Classification: Spirals Dataset","what":"Dataset Description","title":"Benchmarking bnns","text":"dataset generated mlbench.spirals consists points two entangled spirals. sd>0, Gaussian noise added data point.","code":"# Generating the Spirals dataset with Gaussian noise spirals_data <- mlbench.spirals(100, 1.5, 0.05) spirals_data <- cbind.data.frame(y = spirals_data$classes, spirals_data$x) |>   transform(y = as.numeric(y) - 1)  # Converting to binary 0/1  # Splitting the data into training and testing sets (stratified by class) spirals_split <- initial_split(spirals_data, prop = 0.8, strata = \"y\") spirals_train <- training(spirals_split)  # Training data spirals_test <- testing(spirals_split)   # Testing data"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-training-1","dir":"Articles","previous_headings":"Binary Classification: Spirals Dataset","what":"Model Training","title":"Benchmarking bnns","text":"","code":"# Training a Bayesian Neural Network with three hidden layers spirals_bnn <- bnns(y ~ -1 + .,   data = spirals_train, L = 3,   nodes = c(64, 64, 16), act_fn = c(1, 4, 4),   out_act_fn = 2, iter = 1e3, warmup = 2e2 ) #> Trying to compile a simple C file #> Running /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c #> using C compiler: ‘gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0’ #> gcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/home/runner/work/_temp/Library/Rcpp/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/unsupported\"  -I\"/home/runner/work/_temp/Library/BH/include\" -I\"/home/runner/work/_temp/Library/StanHeaders/include/src/\"  -I\"/home/runner/work/_temp/Library/StanHeaders/include/\"  -I\"/home/runner/work/_temp/Library/RcppParallel/include/\"  -I\"/home/runner/work/_temp/Library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o #> In file included from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Core:19, #>                  from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Dense:1, #>                  from /home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22, #>                  from <command-line>: #> /home/runner/work/_temp/Library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory #>   679 | #include <cmath> #>       |          ^~~~~~~ #> compilation terminated. #> make: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1 #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 0.001312 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 13.12 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 1:  #> Chain 1: Gradient evaluation took 0.001698 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 16.98 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 161.082 seconds (Warm-up) #> Chain 2:                686.607 seconds (Sampling) #> Chain 2:                847.689 seconds (Total) #> Chain 2:  #> Chain 1:  #> Chain 1:  Elapsed Time: 160.877 seconds (Warm-up) #> Chain 1:                686.913 seconds (Sampling) #> Chain 1:                847.79 seconds (Total) #> Chain 1:"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-evaluation-1","dir":"Articles","previous_headings":"Binary Classification: Spirals Dataset","what":"Model Evaluation","title":"Benchmarking bnns","text":"","code":"# Making predictions and calculating binary classification metrics (e.g., AUC) spirals_bnn_pred <- predict(spirals_bnn, spirals_test) measure_bin(spirals_test$y, spirals_bnn_pred) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> $conf_mat #>    pred_label #> obs  0  1 #>   0 10  0 #>   1  1  9 #>  #> $accuracy #> [1] 0.95 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 10 controls (obs 0) < 10 cases (obs 1). #> Area under the curve: 0.97 #>  #> $AUC #> [1] 0.97"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-comparison-1","dir":"Articles","previous_headings":"Binary Classification: Spirals Dataset","what":"Model Comparison","title":"Benchmarking bnns","text":"","code":"# Training a random forest model for comparison spirals_rf <- ranger(   y ~ -1 + .,   data = spirals_train |> `colnames<-`(c(\"y\", paste0(\"x\", 1:2))) )  # Evaluating the random forest model spirals_rf_pred <- predict(   spirals_rf,   spirals_test |> `colnames<-`(c(\"y\", paste0(\"x\", 1:2))) ) measure_bin(spirals_test$y, spirals_rf_pred$predictions) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> $conf_mat #>    pred_label #> obs 0 1 #>   0 5 5 #>   1 4 6 #>  #> $accuracy #> [1] 0.55 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 10 controls (obs 0) < 10 cases (obs 1). #> Area under the curve: 0.64 #>  #> $AUC #> [1] 0.64"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"dataset-description-2","dir":"Articles","previous_headings":"Multi-class Classification: Waveform Dataset","what":"Dataset Description","title":"Benchmarking bnns","text":"dataset generated mlbench.waveform consists 21 attributes continuous values variable showing 3 classes (33% 3 classes). class generated combination 2 3 “base” waves.","code":"# Generating the Waveform dataset waveform_data <- mlbench.waveform(100) waveform_data <- cbind.data.frame(y = waveform_data$classes, waveform_data$x) |>   transform(y = as.factor(y))  # Converting the target to a factor  # Splitting the data into training and testing sets (stratified by class) waveform_split <- initial_split(waveform_data, prop = 0.8, strata = \"y\") waveform_train <- training(waveform_split)  # Training data waveform_test <- testing(waveform_split)   # Testing data"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-training-2","dir":"Articles","previous_headings":"Multi-class Classification: Waveform Dataset","what":"Model Training","title":"Benchmarking bnns","text":"","code":"# Training a Bayesian Neural Network with two hidden layers waveform_bnn <- bnns(y ~ -1 + .,   data = waveform_train, L = 2, nodes = c(2, 2),   act_fn = 2:3, out_act_fn = 3, iter = 1e3, warmup = 2e2 ) #> Trying to compile a simple C file #> Running /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c #> using C compiler: ‘gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0’ #> gcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/home/runner/work/_temp/Library/Rcpp/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/unsupported\"  -I\"/home/runner/work/_temp/Library/BH/include\" -I\"/home/runner/work/_temp/Library/StanHeaders/include/src/\"  -I\"/home/runner/work/_temp/Library/StanHeaders/include/\"  -I\"/home/runner/work/_temp/Library/RcppParallel/include/\"  -I\"/home/runner/work/_temp/Library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o #> In file included from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Core:19, #>                  from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Dense:1, #>                  from /home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22, #>                  from <command-line>: #> /home/runner/work/_temp/Library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory #>   679 | #include <cmath> #>       |          ^~~~~~~ #> compilation terminated. #> make: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1 #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.000133 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.33 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 0.000152 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.52 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.662 seconds (Warm-up) #> Chain 2:                2.462 seconds (Sampling) #> Chain 2:                3.124 seconds (Total) #> Chain 2:  #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.792 seconds (Warm-up) #> Chain 1:                3.177 seconds (Sampling) #> Chain 1:                3.969 seconds (Total) #> Chain 1:"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-evaluation-2","dir":"Articles","previous_headings":"Multi-class Classification: Waveform Dataset","what":"Model Evaluation","title":"Benchmarking bnns","text":"","code":"# Making predictions and evaluating multi-class classification metrics waveform_bnn_pred <- predict(waveform_bnn, waveform_test) measure_cat(waveform_test$y, waveform_bnn_pred) #> $log_loss #> [1] 0.4947723 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 3 levels of obs: 1, 2, 3. #> Multi-class area under the curve: 0.9767 #>  #> $AUC #> [1] 0.9766865"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"model-comparison-2","dir":"Articles","previous_headings":"Multi-class Classification: Waveform Dataset","what":"Model Comparison","title":"Benchmarking bnns","text":"","code":"# Training a random forest model with probability outputs for comparison waveform_rf <- ranger(   y ~ -1 + .,   data = waveform_train |> `colnames<-`(c(\"y\", paste0(\"x\", 1:21))),   probability = TRUE )  # Evaluating the random forest model waveform_rf_pred <- predict(   waveform_rf,   waveform_test |> `colnames<-`(c(\"y\", paste0(\"x\", 1:21))) ) measure_cat(waveform_test$y, waveform_rf_pred$predictions) #> $log_loss #>         1  #> 0.5073519  #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 3 levels of obs: 1, 2, 3. #> Multi-class area under the curve: 0.9742 #>  #> $AUC #> [1] 0.9742063"},{"path":"https://swarnendu-stat.github.io/bnns/articles/benchmarking.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Benchmarking bnns","text":"bnns package showcases strong predictive performance across regression, binary classification, multi-class classification tasks. addition accurate predictions, provides posterior distributions, enabling: Uncertainty Quantification: Offers insights confidence predictions, crucial high-stakes applications like clinical trials finance. Probabilistic Decision-Making: Facilitates decisions uncertainty integrating Bayesian principles. Model Comparisons: Demonstrates comparable performance ranger package, added advantage interpretability Bayesian inference. Overall, bnns powerful tool tasks requiring predictive accuracy interpretability, making suitable various domains.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"bnns","text":"bnns package provides efficient user-friendly implementation Bayesian Neural Networks (BNNs) regression, binary classification, multiclass classification problems. integrating Bayesian inference, bnns allows uncertainty quantification predictions robust parameter estimation. vignette covers: 1. Installing loading package 2. Preparing data 3. Fitting BNN model 4. Summarizing model 5. Making predictions 6. Model evaluation 7. Customizing prior","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"installation","dir":"Articles","previous_headings":"","what":"1. Installation","title":"bnns","text":"install package, use following commands: Load package R session:","code":"# Install from CRAN (if available) #  install.packages(\"bnns\")  # Or install the development version from GitHub # devtools::install_github(\"swarnendu-stat/bnns\") library(bnns)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"preparing-the-data","dir":"Articles","previous_headings":"","what":"2. Preparing the Data","title":"bnns","text":"bnns package expects data form matrices predictors vector responses. ’s example generating synthetic data: binary multiclass classification:","code":"# Generate training data set.seed(123) df <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) # Binary classification response df$y_bin <- sample(0:1, 10, replace = TRUE)  # Multiclass classification response df$y_cat <- factor(sample(letters[1:3], 10, replace = TRUE)) # 3 classes"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"fitting-a-bayesian-neural-network-model","dir":"Articles","previous_headings":"","what":"3. Fitting a Bayesian Neural Network Model","title":"bnns","text":"Fit Bayesian Neural Network using bnns() function. Specify network architecture using arguments like number layers (L), nodes per layer (nodes), activation functions (act_fn).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"regression-example","dir":"Articles","previous_headings":"3. Fitting a Bayesian Neural Network Model","what":"Regression Example","title":"bnns","text":"","code":"model_reg <- bnns(   y ~ -1 + x1 + x2,   data = df,   L = 2, # Number of hidden layers   nodes = c(16, 8), # Nodes per layer   act_fn = c(2, 3), # Activation functions: 2 = Sigmoid, 3 = ReLU   out_act_fn = 1, # Output activation function: 1 = Identity (for regression)   iter = 2e2,   warmup = 1e2,   chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"binary-classification-example","dir":"Articles","previous_headings":"3. Fitting a Bayesian Neural Network Model","what":"Binary Classification Example","title":"bnns","text":"","code":"model_bin <- bnns(   y_bin ~ -1 + x1 + x2,   data = df,   L = 1,   nodes = c(16),   act_fn = c(2),   out_act_fn = 2, # Output activation: 2 = Logistic sigmoid   iter = 2e2,   warmup = 1e2,   chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"multiclass-classification-example","dir":"Articles","previous_headings":"3. Fitting a Bayesian Neural Network Model","what":"Multiclass Classification Example","title":"bnns","text":"","code":"model_cat <- bnns(   y_cat ~ -1 + x1 + x2,   data = df,   L = 3,   nodes = c(32, 16, 8),   act_fn = c(3, 2, 2),   out_act_fn = 3, # Output activation: 3 = Softmax   iter = 2e2,   warmup = 1e2,   chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"summarizing-the-model","dir":"Articles","previous_headings":"","what":"4. Summarizing the Model","title":"bnns","text":"Use summary() function view details fitted model, including network architecture, posterior distributions, predictive performance.","code":"summary(model_reg) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = df, L = 2, nodes = c(16,  #>     8), act_fn = c(2, 3), out_act_fn = 1, iter = 200, warmup = 100,  #>     chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 2  #> Nodes per layer: 16, 8  #> Activation functions: 2, 3  #> Output activation function: 1  #>  #> Posterior Summary (Key Parameters): #>                 mean    se_mean        sd       2.5%        25%         50% #> w_out[1] -0.07970081 0.06846247 0.9682056 -2.1360113 -0.6963688 -0.16679456 #> w_out[2]  0.06456191 0.10749989 1.0291658 -2.1225425 -0.5663087  0.11947689 #> w_out[3]  0.11422267 0.06824486 0.8453540 -1.3298244 -0.5145013  0.14489377 #> w_out[4] -0.03942275 0.07463729 0.7954668 -1.4593665 -0.6167020 -0.00268951 #> w_out[5]  0.05517880 0.11031575 0.9477827 -1.6628032 -0.6993092  0.06904414 #> w_out[6] -0.10051502 0.08215889 0.8566549 -1.6301129 -0.6097178 -0.10033871 #> w_out[7]  0.01175058 0.08029138 0.9024368 -2.0663648 -0.4880417  0.06540799 #> w_out[8] -0.13108646 0.07697444 0.8361898 -1.5942451 -0.7825919 -0.12908555 #> b_out     0.01766977 0.10965748 1.0232441 -1.7807058 -0.6878054  0.10364959 #> sigma     0.82683004 0.02089225 0.2432927  0.4965386  0.6407224  0.79565441 #>                75%    97.5%     n_eff      Rhat #> w_out[1] 0.6512718 1.518476 200.00000 0.9912740 #> w_out[2] 0.7142186 1.789971  91.65469 0.9899925 #> w_out[3] 0.6141343 2.047458 153.43956 0.9899708 #> w_out[4] 0.4394828 1.363184 113.58801 0.9932006 #> w_out[5] 0.6542098 1.937073  73.81464 1.0061040 #> w_out[6] 0.3287825 1.773595 108.71831 0.9908565 #> w_out[7] 0.5665454 1.653195 126.32686 0.9920570 #> w_out[8] 0.4291506 1.343326 118.00941 0.9906425 #> b_out    0.6020728 1.700598  87.07270 0.9899495 #> sigma    0.9639360 1.298553 135.60875 0.9978727 #>  #> Model Fit Information: #> Iterations: 200  #> Warmup: 100  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> RMSE (training): 0.6187873  #> MAE (training): 0.5015489  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values. summary(model_bin) #> Call: #> bnns.default(formula = y_bin ~ -1 + x1 + x2, data = df, L = 1,  #>     nodes = c(16), act_fn = c(2), out_act_fn = 2, iter = 200,  #>     warmup = 100, chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 1  #> Nodes per layer: 16  #> Activation functions: 2  #> Output activation function: 2  #>  #> Posterior Summary (Key Parameters): #>                  mean    se_mean        sd      2.5%        25%         50% #> w_out[1]  -0.10110551 0.07690621 0.8918423 -1.695352 -0.8001027  0.01444756 #> w_out[2]  -0.08726693 0.06605833 0.9342059 -1.774685 -0.7822605 -0.17450266 #> w_out[3]  -0.18923401 0.06313568 0.8928733 -1.948382 -0.6916708 -0.06156762 #> w_out[4]  -0.13519437 0.07263513 0.9546914 -2.131777 -0.7984818 -0.10924737 #> w_out[5]  -0.05836183 0.06911558 0.8600123 -1.601996 -0.6339802 -0.04144955 #> w_out[6]  -0.11256831 0.07408610 1.0477356 -2.115622 -0.9079361 -0.05380241 #> w_out[7]  -0.11107491 0.07208329 0.8974309 -1.745066 -0.7319325 -0.08583247 #> w_out[8]  -0.05894826 0.07540536 1.0663929 -2.185835 -0.7446250 -0.01440285 #> w_out[9]  -0.11478543 0.07315091 1.0345101 -2.252466 -0.7495023 -0.08728692 #> w_out[10]  0.03207499 0.07732217 0.9144306 -1.517396 -0.6228746 -0.01176603 #> w_out[11] -0.12440063 0.06514516 0.9212917 -1.963137 -0.7124421 -0.06475202 #> w_out[12] -0.15843373 0.07875792 1.1138051 -2.430101 -0.6615781 -0.01753044 #> w_out[13] -0.14744805 0.06857286 0.9697667 -2.189953 -0.7890001 -0.11954830 #> w_out[14] -0.22538573 0.06788229 0.7980628 -1.696545 -0.7452381 -0.22185988 #> w_out[15] -0.15469604 0.06234291 0.8816618 -1.782075 -0.7271137 -0.11837500 #> w_out[16] -0.07055980 0.05682076 0.8035668 -1.604959 -0.5366324 -0.08063014 #> b_out     -0.14244196 0.05992138 0.8474163 -1.786832 -0.6901619 -0.04366264 #>                 75%    97.5%    n_eff      Rhat #> w_out[1]  0.5388349 1.504491 134.4787 0.9899641 #> w_out[2]  0.6312955 1.745942 200.0000 0.9919117 #> w_out[3]  0.3466554 1.456184 200.0000 0.9991987 #> w_out[4]  0.3672884 1.744327 172.7558 0.9900617 #> w_out[5]  0.4294623 1.613986 154.8308 0.9900073 #> w_out[6]  0.6569482 1.649661 200.0000 0.9925097 #> w_out[7]  0.5617465 1.533728 155.0004 0.9922798 #> w_out[8]  0.6694988 1.876662 200.0000 1.0106131 #> w_out[9]  0.5167634 1.866479 200.0000 0.9902438 #> w_out[10] 0.6673431 1.826614 139.8600 0.9900235 #> w_out[11] 0.4033759 1.279229 200.0000 0.9917090 #> w_out[12] 0.4383581 1.780021 200.0000 0.9949652 #> w_out[13] 0.4366914 1.792881 200.0000 0.9928818 #> w_out[14] 0.4086064 1.207952 138.2169 1.0139442 #> w_out[15] 0.3417129 1.625343 200.0000 0.9948078 #> w_out[16] 0.4470843 1.317208 200.0000 1.0030229 #> b_out     0.4753426 1.308099 200.0000 0.9900014 #>  #> Model Fit Information: #> Iterations: 200  #> Warmup: 100  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> Confusion matrix (training with 0.5 cutoff): 7 3  #> Accuracy (training with 0.5 cutoff): 0.7  #> AUC (training): 0.6666667  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values. summary(model_cat) #> Call: #> bnns.default(formula = y_cat ~ -1 + x1 + x2, data = df, L = 3,  #>     nodes = c(32, 16, 8), act_fn = c(3, 2, 2), out_act_fn = 3,  #>     iter = 200, warmup = 100, chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 3  #> Nodes per layer: 32, 16, 8  #> Activation functions: 3, 2, 2  #> Output activation function: 3  #>  #> Posterior Summary (Key Parameters): #>                     mean    se_mean        sd      2.5%        25%          50% #> w_out[1,1]  0.1445477331 0.05847329 0.8269372 -1.456178 -0.5072202  0.075083204 #> w_out[1,2] -0.0512763363 0.07053282 0.9974847 -1.826686 -0.6343180 -0.006513991 #> w_out[1,3]  0.0344279207 0.07382964 1.0441088 -1.900945 -0.7126314  0.054312537 #> w_out[2,1]  0.0577968580 0.09996247 1.3380105 -2.578655 -0.6611543  0.086531062 #> w_out[2,2] -0.0957887406 0.06658559 0.9416625 -1.629212 -0.7497064 -0.030971708 #> w_out[2,3]  0.0029893747 0.07462410 0.9998355 -2.046424 -0.8824320 -0.001885644 #> w_out[3,1]  0.1368857790 0.08409256 1.1892483 -2.049067 -0.6705226 -0.084743369 #> w_out[3,2] -0.1021233280 0.08594312 1.1181669 -2.321632 -0.7664821 -0.107589165 #> w_out[3,3]  0.0260325906 0.06655051 0.9411663 -1.788178 -0.4832624 -0.018023712 #> w_out[4,1] -0.0028844054 0.06693483 0.9466014 -1.712807 -0.7151777 -0.012884313 #> w_out[4,2]  0.0001968483 0.08160187 1.1148632 -2.276743 -0.7590366  0.011934448 #> w_out[4,3] -0.0045651291 0.07408571 1.0477302 -1.832875 -0.6487458 -0.019175158 #> w_out[5,1]  0.0950744976 0.09319237 1.0716226 -1.840047 -0.6311567  0.091425026 #> w_out[5,2] -0.1155975735 0.06978515 0.9650066 -1.741906 -0.8096174 -0.078521709 #> w_out[5,3]  0.0669772476 0.08491057 0.8977346 -1.684854 -0.5377517  0.049692098 #> w_out[6,1]  0.1231706480 0.08119341 0.9705829 -1.624344 -0.5028565  0.083643732 #> w_out[6,2] -0.1589556537 0.08022912 0.8818673 -1.621459 -0.8040660 -0.130048250 #> w_out[6,3]  0.0510863392 0.07949673 0.9623707 -2.086645 -0.4501735  0.041080257 #> w_out[7,1]  0.1730161358 0.08462972 0.9902159 -1.577846 -0.6616558  0.291902049 #> w_out[7,2] -0.0295854306 0.06957140 0.9838882 -1.595405 -0.7977793 -0.115578889 #> w_out[7,3]  0.0379173135 0.06643207 0.9394914 -1.716310 -0.5951511  0.175860162 #> w_out[8,1]  0.1331306062 0.07408174 0.8789793 -1.749949 -0.3459968  0.192039766 #> w_out[8,2] -0.0730116138 0.07666538 0.8912249 -1.742001 -0.6934322 -0.086344702 #> w_out[8,3]  0.0394090795 0.08023090 0.9775812 -1.697123 -0.7157675  0.111447099 #> b_out[1]    0.1792094227 0.05526818 0.7816102 -1.431437 -0.3094301  0.193881645 #> b_out[2]   -0.0519924122 0.06359956 0.8994337 -1.817950 -0.6075309 -0.109846196 #> b_out[3]    0.0934671540 0.06677003 0.8584870 -1.447075 -0.4450014  0.100044898 #>                  75%    97.5%    n_eff      Rhat #> w_out[1,1] 0.8436876 1.542492 200.0000 0.9914225 #> w_out[1,2] 0.5894057 1.812237 200.0000 1.0022855 #> w_out[1,3] 0.8362871 1.804830 200.0000 0.9937128 #> w_out[2,1] 0.9872535 2.318638 179.1617 0.9900582 #> w_out[2,2] 0.4358062 1.507324 200.0000 0.9914146 #> w_out[2,3] 0.7445109 1.799740 179.5142 1.0032525 #> w_out[3,1] 0.9938209 2.597693 200.0000 1.0241183 #> w_out[3,2] 0.8077423 1.785540 169.2743 0.9924486 #> w_out[3,3] 0.5610106 2.028296 200.0000 1.0022836 #> w_out[4,1] 0.6299420 1.713447 200.0000 1.0019255 #> w_out[4,2] 0.9185308 1.952063 186.6564 0.9934494 #> w_out[4,3] 0.5901454 1.975573 200.0000 1.0013152 #> w_out[5,1] 0.6765450 2.259248 132.2279 0.9913469 #> w_out[5,2] 0.5971397 1.741868 191.2205 0.9900270 #> w_out[5,3] 0.5486228 1.950470 111.7821 0.9957489 #> w_out[6,1] 0.7216515 1.927545 142.8972 0.9939585 #> w_out[6,2] 0.4411440 1.509997 120.8210 0.9920938 #> w_out[6,3] 0.6591789 1.596097 146.5501 0.9902139 #> w_out[7,1] 0.7530175 1.959700 136.9033 0.9904907 #> w_out[7,2] 0.6477262 1.790560 200.0000 0.9902553 #> w_out[7,3] 0.7638280 1.718886 200.0000 1.0173475 #> w_out[8,1] 0.7267266 1.617212 140.7781 1.0023445 #> w_out[8,2] 0.5629822 1.524652 135.1375 0.9976894 #> w_out[8,3] 0.7581010 1.992828 148.4644 1.0253875 #> b_out[1]   0.6915429 1.608632 200.0000 0.9926552 #> b_out[2]   0.4992708 1.642560 200.0000 0.9910938 #> b_out[3]   0.6969623 1.617561 165.3120 0.9918786 #>  #> Model Fit Information: #> Iterations: 200  #> Warmup: 100  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> Log-loss (training): 0.9418529  #> AUC (training): 0.9111111  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values."},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"making-predictions","dir":"Articles","previous_headings":"","what":"5. Making Predictions","title":"bnns","text":"predict() function generates predictions new data. format predictions depends output activation function.","code":"# New data test_x <- matrix(runif(10), nrow = 5, ncol = 2) |>   data.frame() |>   `colnames<-`(c(\"x1\", \"x2\"))  # Regression predictions pred_reg <- predict(model_reg, test_x)  # Binary classification predictions pred_bin <- predict(model_bin, test_x)  # Multiclass classification predictions pred_cat <- predict(model_cat, test_x)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"evaluating-the-model","dir":"Articles","previous_headings":"","what":"6. Evaluating the Model","title":"bnns","text":"bnns package includes utility functions like measure_cont, measure_bin, measure_cat evaluating model performance.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"regression-evaluation","dir":"Articles","previous_headings":"6. Evaluating the Model","what":"Regression Evaluation","title":"bnns","text":"","code":"# True responses test_y <- rnorm(5)  # Evaluate predictions metrics_reg <- measure_cont(obs = test_y, pred = pred_reg) print(metrics_reg) #> $rmse #> [1] 1.419593 #>  #> $mae #> [1] 1.377375"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"binary-classification-evaluation","dir":"Articles","previous_headings":"6. Evaluating the Model","what":"Binary Classification Evaluation","title":"bnns","text":"","code":"# True responses test_y_bin <- sample(c(rep(0, 2), rep(1, 3)), 5)  # Evaluate predictions metrics_bin <- measure_bin(obs = test_y_bin, pred = pred_bin) #> Setting levels: control = 0, case = 1 #> Setting direction: controls > cases print(metrics_bin) #> $conf_mat #>    pred_label #> obs 0 #>   0 2 #>   1 3 #>  #> $accuracy #> [1] 0.4 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 2 controls (obs 0) > 3 cases (obs 1). #> Area under the curve: 0.6667 #>  #> $AUC #> [1] 0.6666667"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"multiclass-classification-evaluation","dir":"Articles","previous_headings":"6. Evaluating the Model","what":"Multiclass Classification Evaluation","title":"bnns","text":"","code":"# True responses test_y_cat <- factor(sample(letters[1:3], 5, replace = TRUE))  # Evaluate predictions metrics_cat <- measure_cat(obs = test_y_cat, pred = pred_cat) #> Warning in multiclass_roc_multivariate(response, predictor, levels, percent, : #> The following classes were not found in 'response': NA. print(metrics_cat) #> $log_loss #> [1] 1.358409 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 2 levels of obs: b, c. #> Multi-class area under the curve: 1 #>  #> $AUC #> [1] 1"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"customized-prior","dir":"Articles","previous_headings":"","what":"7. Customized Prior","title":"bnns","text":"Customized priors can used weights well sigma parameter (regression). show example use Cauchy prior weights multi-classification case.","code":"model_cat_cauchy <- bnns(   y_cat ~ -1 + x1 + x2,   data = df,   L = 3,   nodes = c(32, 16, 8),   act_fn = c(3, 2, 2),   out_act_fn = 3, # Output activation: 3 = Softmax   iter = 2e2,   warmup = 1e2,   chains = 1,   prior_weights = list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) ) # Evaluate predictions metrics_cat_cauchy <- measure_cat(obs = test_y_cat, pred = predict(model_cat_cauchy, test_x)) #> Warning in multiclass_roc_multivariate(response, predictor, levels, percent, : #> The following classes were not found in 'response': NA. print(metrics_cat_cauchy) #> $log_loss #> [1] 1.483828 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 2 levels of obs: b, c. #> Multi-class area under the curve: 0.625 #>  #> $AUC #> [1] 0.625"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"notes-on-bayesian-neural-networks","dir":"Articles","previous_headings":"","what":"8. Notes on Bayesian Neural Networks","title":"bnns","text":"Bayesian inference allows use prior knowledge weights. allows uncertainty quantification predictions. Always check convergence diagnostics R-hat values. Use informative priors possible stabilize model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"bnns","text":"details, consult source code GitHub.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Common Application Areas of bnns","text":"article demonstrates use bnns package three datasets mlbench package: Regression: BostonHousing dataset Binary Classification: PimaIndiansDiabetes dataset Multi-class Classification: Glass dataset dataset, : 1. Prepare data training testing. 2. Build Bayesian Neural Network using bnns package. 3. Evaluate model’s predictive performance.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"dataset-description","dir":"Articles","previous_headings":"Regression: BostonHousing Dataset","what":"Dataset Description","title":"Common Application Areas of bnns","text":"BostonHousing dataset contains information housing prices Boston, features like crime rate, average number rooms, .","code":"data(BostonHousing) BH_data <- BostonHousing # Splitting data into training and testing sets BH_split <- initial_split(BH_data, prop = 0.8) BH_train <- training(BH_split) BH_test <- testing(BH_split)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"model-training","dir":"Articles","previous_headings":"Regression: BostonHousing Dataset","what":"Model Training","title":"Common Application Areas of bnns","text":"","code":"model_reg <- bnns(   medv ~ -1 + .,   data = BH_train, L = 2, out_act_fn = 1,   iter = 1e3, warmup = 2e2, chains = 2, cores = 2 ) #> Trying to compile a simple C file #> Running /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c #> using C compiler: ‘gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0’ #> gcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/home/runner/work/_temp/Library/Rcpp/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/unsupported\"  -I\"/home/runner/work/_temp/Library/BH/include\" -I\"/home/runner/work/_temp/Library/StanHeaders/include/src/\"  -I\"/home/runner/work/_temp/Library/StanHeaders/include/\"  -I\"/home/runner/work/_temp/Library/RcppParallel/include/\"  -I\"/home/runner/work/_temp/Library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o #> In file included from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Core:19, #>                  from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Dense:1, #>                  from /home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22, #>                  from <command-line>: #> /home/runner/work/_temp/Library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory #>   679 | #include <cmath> #>       |          ^~~~~~~ #> compilation terminated. #> make: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1 #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.000356 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.56 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 0.000345 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 3.45 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 15.592 seconds (Warm-up) #> Chain 2:                47.989 seconds (Sampling) #> Chain 2:                63.581 seconds (Total) #> Chain 2:  #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 18.346 seconds (Warm-up) #> Chain 1:                80.253 seconds (Sampling) #> Chain 1:                98.599 seconds (Total) #> Chain 1:"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"model-evaluation","dir":"Articles","previous_headings":"Regression: BostonHousing Dataset","what":"Model Evaluation","title":"Common Application Areas of bnns","text":"","code":"BH_pred <- predict(model_reg, newdata = BH_test) measure_cont(BH_test$medv, BH_pred) #> $rmse #> [1] 8.297719 #>  #> $mae #> [1] 5.822318"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"dataset-description-1","dir":"Articles","previous_headings":"Binary Classification: PimaIndiansDiabetes Dataset","what":"Dataset Description","title":"Common Application Areas of bnns","text":"PimaIndiansDiabetes dataset contains features related health status predicting presence diabetes.","code":"data(PimaIndiansDiabetes) PID_data <- PimaIndiansDiabetes |>   transform(diabetes = ifelse(diabetes == \"pos\", 1, 0)) # Splitting data into training and testing sets PID_split <- initial_split(PID_data, prop = 0.8, strata = \"diabetes\") PID_train <- training(PID_split) PID_test <- testing(PID_split)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"model-training-1","dir":"Articles","previous_headings":"Binary Classification: PimaIndiansDiabetes Dataset","what":"Model Training","title":"Common Application Areas of bnns","text":"","code":"model_bin <- bnns(   diabetes ~ -1 + .,   data = PID_train, L = 2,   out_act_fn = 2, iter = 1e3, warmup = 2e2, chains = 2, cores = 2 ) #> Trying to compile a simple C file #> Running /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c #> using C compiler: ‘gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0’ #> gcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/home/runner/work/_temp/Library/Rcpp/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/unsupported\"  -I\"/home/runner/work/_temp/Library/BH/include\" -I\"/home/runner/work/_temp/Library/StanHeaders/include/src/\"  -I\"/home/runner/work/_temp/Library/StanHeaders/include/\"  -I\"/home/runner/work/_temp/Library/RcppParallel/include/\"  -I\"/home/runner/work/_temp/Library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o #> In file included from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Core:19, #>                  from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Dense:1, #>                  from /home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22, #>                  from <command-line>: #> /home/runner/work/_temp/Library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory #>   679 | #include <cmath> #>       |          ^~~~~~~ #> compilation terminated. #> make: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1 #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.000454 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.54 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 0.000447 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.47 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 24.281 seconds (Warm-up) #> Chain 2:                63.066 seconds (Sampling) #> Chain 2:                87.347 seconds (Total) #> Chain 2:  #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 22.337 seconds (Warm-up) #> Chain 1:                82.138 seconds (Sampling) #> Chain 1:                104.475 seconds (Total) #> Chain 1:"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"model-evaluation-1","dir":"Articles","previous_headings":"Binary Classification: PimaIndiansDiabetes Dataset","what":"Model Evaluation","title":"Common Application Areas of bnns","text":"","code":"PID_pred <- predict(model_bin, newdata = PID_test) PID_measure <- measure_bin(PID_test$diabetes, PID_pred) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases PID_measure #> $conf_mat #>    pred_label #> obs  0  1 #>   0 82 18 #>   1 29 25 #>  #> $accuracy #> [1] 0.6948052 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 100 controls (obs 0) < 54 cases (obs 1). #> Area under the curve: 0.7296 #>  #> $AUC #> [1] 0.7296296 plot(PID_measure$ROC)"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"dataset-description-2","dir":"Articles","previous_headings":"Multi-class Classification: Glass Dataset","what":"Dataset Description","title":"Common Application Areas of bnns","text":"Glass dataset contains features classify glass types.","code":"data(Glass) Glass_data <- Glass  # Splitting data into training and testing sets Glass_split <- initial_split(Glass_data, prop = 0.8, strata = \"Type\") Glass_train <- training(Glass_split) Glass_test <- testing(Glass_split)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"model-training-2","dir":"Articles","previous_headings":"Multi-class Classification: Glass Dataset","what":"Model Training","title":"Common Application Areas of bnns","text":"","code":"model_multi <- bnns(   Type ~ -1 + .,   data = Glass_train, L = 2,   out_act_fn = 3, iter = 1e3, warmup = 2e2, chains = 2, cores = 2 ) #> Trying to compile a simple C file #> Running /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c #> using C compiler: ‘gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0’ #> gcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/home/runner/work/_temp/Library/Rcpp/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/\"  -I\"/home/runner/work/_temp/Library/RcppEigen/include/unsupported\"  -I\"/home/runner/work/_temp/Library/BH/include\" -I\"/home/runner/work/_temp/Library/StanHeaders/include/src/\"  -I\"/home/runner/work/_temp/Library/StanHeaders/include/\"  -I\"/home/runner/work/_temp/Library/RcppParallel/include/\"  -I\"/home/runner/work/_temp/Library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o #> In file included from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Core:19, #>                  from /home/runner/work/_temp/Library/RcppEigen/include/Eigen/Dense:1, #>                  from /home/runner/work/_temp/Library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22, #>                  from <command-line>: #> /home/runner/work/_temp/Library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory #>   679 | #include <cmath> #>       |          ^~~~~~~ #> compilation terminated. #> make: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1 #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.000357 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.57 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 0.000345 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 3.45 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 201 / 1000 [ 20%]  (Sampling) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Sampling) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 17.43 seconds (Warm-up) #> Chain 1:                55.327 seconds (Sampling) #> Chain 1:                72.757 seconds (Total) #> Chain 1:  #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 19.259 seconds (Warm-up) #> Chain 2:                86.643 seconds (Sampling) #> Chain 2:                105.902 seconds (Total) #> Chain 2:"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"model-evaluation-2","dir":"Articles","previous_headings":"Multi-class Classification: Glass Dataset","what":"Model Evaluation","title":"Common Application Areas of bnns","text":"","code":"Glass_pred <- predict(model_multi, newdata = Glass_test) measure_cat(Glass_test$Type, Glass_pred) #> $log_loss #> [1] 1.353789 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 6 levels of obs: 1, 2, 3, 5, 6, 7. #> Multi-class area under the curve: 0.7095 #>  #> $AUC #> [1] 0.7094742"},{"path":"https://swarnendu-stat.github.io/bnns/articles/common_applications.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Common Application Areas of bnns","text":"performance bnns package demonstrates flexibility across various machine learning tasks. provides posterior distributions predictions, can used uncertainty quantification probabilistic decision-making.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Bayesian Neural Networks (BNNs) offer robust framework prediction clinical trials providing posterior distributions predictions. allows probabilistic reasoning, computing probability treatment achieves certain efficacy threshold proportion success. vignette, : 1. Illustrate data preparation clinical trial setting. 2. Fit BNN simulate clinical trial outcomes. 3. Leverage posterior distributions decision-making, calculating posterior probabilities treatment success.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"1. Data Preparation","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Consider hypothetical clinical trial comparing efficacy new treatment placebo. response variable binary, representing treatment success (1) failure (0).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"simulating-data","dir":"Articles","previous_headings":"1. Data Preparation","what":"Simulating Data","title":"Using Bayesian Neural Networks in Clinical Trials","text":"","code":"set.seed(123)  # Simulate predictor variables (e.g., patient covariates) n_subjects <- 100 Age <- runif(n_subjects, 18, 50) # Age in years Dose <- runif(n_subjects, 10, 100) # Dose levels Severity <- runif(n_subjects, 1, 10) # Baseline severity (arbitrary scale)  # Define true probabilities using a nonlinear function beta_0 <- 1 beta_1 <- 0.3 beta_2 <- -0.1 beta_3 <- -0.02 beta_4 <- 0.005  logit_p <- beta_0 + beta_1 * Dose + beta_2 * log(Severity) +   beta_3 * Age^2 + beta_4 * (Age * Dose) p_success <- 1 / (1 + exp(-logit_p)) # Sigmoid transformation  # Simulate binary outcomes Success <- rbinom(n_subjects, size = 1, prob = p_success)  trial_data <- cbind.data.frame(Success, Age, Dose, Severity)  # Split into training and testing train_idx <- sample(seq_len(n_subjects), size = 0.8 * n_subjects) training_data <- trial_data[train_idx, ] test_data <- trial_data[-train_idx, ]"},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"fitting-a-bayesian-neural-network","dir":"Articles","previous_headings":"","what":"2. Fitting a Bayesian Neural Network","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Fit BNN simulated data. use binary classification model logistic sigmoid activation output layer.","code":"# Fit a BNN model <- bnns(   formula = Success ~ -1 + .,   data = training_data,   L = 2, # Number of hidden layers   nodes = c(16, 8), # Nodes per layer   act_fn = c(2, 2), # Activation functions for hidden layers   out_act_fn = 2, # Output activation: logistic sigmoid   iter = 2e2, # Bayesian sampling iterations   warmup = 1e2, # Warmup iterations   chains = 1 # Number of MCMC chains )"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"generating-predictions-with-uncertainty","dir":"Articles","previous_headings":"3. Posterior Predictions","what":"Generating Predictions with Uncertainty","title":"Using Bayesian Neural Networks in Clinical Trials","text":"posterior distribution predictions allows us compute just point estimates also probabilistic metrics. entry posterior_preds represents predicted probability success single posterior sample.","code":"# Generate posterior predictions for the test set posterior_preds <- predict(model, subset(test_data, select = -Success)) head(posterior_preds) # Each row corresponds to a subject, and columns are MCMC samples #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.9062137 0.8335529 0.9401079 0.9590640 0.9919739 0.8722404 0.8982879 #> [2,] 0.4204703 0.4115653 0.2476447 0.5147932 0.4085100 0.2640329 0.4539966 #> [3,] 0.7477193 0.7733507 0.6951838 0.9514166 0.9274107 0.7678251 0.7722048 #> [4,] 0.8732688 0.8183802 0.9396331 0.9285820 0.9895322 0.8773471 0.9032350 #> [5,] 0.2170031 0.3257125 0.3320307 0.4219474 0.3725069 0.2507164 0.3133900 #> [6,] 0.3199969 0.4518530 0.3819711 0.5980656 0.5418036 0.3403906 0.4108854 #>           [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14] #> [1,] 0.9677636 0.8524162 0.8522924 0.8184400 0.9657593 0.8784986 0.8201273 #> [2,] 0.3542480 0.6205557 0.2789536 0.2320280 0.3131029 0.3633745 0.2434076 #> [3,] 0.7112474 0.6886955 0.8120000 0.6944182 0.9178727 0.4684934 0.7760065 #> [4,] 0.9621285 0.8480691 0.8475879 0.8211074 0.9551138 0.8696047 0.7646138 #> [5,] 0.4340413 0.3141186 0.3403176 0.2736207 0.4144107 0.2954323 0.4915524 #> [6,] 0.5470473 0.4462321 0.4684776 0.3121366 0.4778728 0.3229921 0.5530188 #>          [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21] #> [1,] 0.9593908 0.9356778 0.8546193 0.8925409 0.8758250 0.7631612 0.9567358 #> [2,] 0.3527832 0.4720721 0.2328137 0.1968743 0.2089155 0.2743101 0.6100914 #> [3,] 0.5792619 0.5351346 0.5133759 0.6606584 0.6041600 0.8657223 0.9072963 #> [4,] 0.9242324 0.9272583 0.8542228 0.8353541 0.8357104 0.7177186 0.9477703 #> [5,] 0.1511894 0.2339606 0.1817854 0.2459936 0.1125821 0.5527613 0.1567513 #> [6,] 0.3234398 0.3287756 0.3101685 0.3690858 0.1844588 0.6816026 0.3237377 #>          [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28] #> [1,] 0.9331689 0.9329158 0.9426554 0.9044432 0.9111997 0.9121889 0.8625658 #> [2,] 0.3769680 0.1644491 0.3068896 0.4850575 0.3942688 0.4497209 0.3828122 #> [3,] 0.7829915 0.7204448 0.8347028 0.7750339 0.9189465 0.8568211 0.8587497 #> [4,] 0.9230535 0.9307014 0.8962607 0.8921747 0.8988923 0.8243679 0.6926867 #> [5,] 0.3673487 0.2486405 0.2797588 0.3642235 0.6172400 0.4568082 0.6192524 #> [6,] 0.4598175 0.3549363 0.4110749 0.4230202 0.7239343 0.6282899 0.7178172 #>          [,29]     [,30]     [,31]     [,32]     [,33]     [,34]     [,35] #> [1,] 0.8484791 0.9542246 0.9602404 0.9357228 0.9394700 0.8199521 0.9797988 #> [2,] 0.4228981 0.3841118 0.6776734 0.8269675 0.6018438 0.3761443 0.3498603 #> [3,] 0.8445934 0.9427342 0.8775716 0.8878071 0.9085519 0.9216222 0.9516617 #> [4,] 0.8036760 0.8762663 0.9620489 0.8437045 0.8483583 0.7904393 0.9490875 #> [5,] 0.3185250 0.3564466 0.5361770 0.4396265 0.4073050 0.4329073 0.5254285 #> [6,] 0.3964019 0.3995538 0.6545576 0.5689066 0.5365180 0.5176712 0.7143767 #>          [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42] #> [1,] 0.9534645 0.8758179 0.9062767 0.9279392 0.9381318 0.9613728 0.9362955 #> [2,] 0.1058935 0.3729644 0.5259679 0.4966943 0.7239841 0.2235967 0.4080229 #> [3,] 0.4957216 0.7689832 0.7725000 0.8993579 0.8866630 0.8056729 0.9423801 #> [4,] 0.9258469 0.8721311 0.8945832 0.8851561 0.9247410 0.9503497 0.9287343 #> [5,] 0.1349837 0.2079545 0.3296129 0.3039564 0.4708130 0.3995912 0.3892443 #> [6,] 0.1543610 0.3386800 0.3677774 0.4277087 0.6304738 0.5136836 0.5990180 #>          [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49] #> [1,] 0.9669054 0.9177812 0.9502537 0.9306638 0.9764351 0.9538929 0.9132688 #> [2,] 0.3695236 0.3268315 0.5038306 0.5267577 0.5271400 0.7573290 0.4915736 #> [3,] 0.9482543 0.8274708 0.7791003 0.8055793 0.9225364 0.9501151 0.7538855 #> [4,] 0.9273490 0.9158460 0.9269720 0.7691952 0.9355604 0.9435723 0.9098030 #> [5,] 0.3660159 0.5031485 0.2967139 0.2819297 0.4465772 0.3450042 0.3972591 #> [6,] 0.5395314 0.6011345 0.3874847 0.3946501 0.4947116 0.5505594 0.4966650 #>          [,50]     [,51]     [,52]     [,53]     [,54]     [,55]     [,56] #> [1,] 0.9278026 0.9802747 0.8942321 0.8755287 0.9247451 0.9671279 0.9314521 #> [2,] 0.3956324 0.6121979 0.5991980 0.5617213 0.5307114 0.1738080 0.3891865 #> [3,] 0.6920177 0.8974057 0.9342559 0.7561331 0.7800093 0.8845034 0.8428539 #> [4,] 0.9060591 0.9773210 0.8525200 0.8250347 0.9284750 0.9691098 0.9288824 #> [5,] 0.2285491 0.3752401 0.4826738 0.4501153 0.2187272 0.5791478 0.4357125 #> [6,] 0.2546649 0.4098976 0.5459098 0.4671363 0.4021149 0.6863167 0.5310861 #>          [,57]     [,58]     [,59]     [,60]     [,61]     [,62]     [,63] #> [1,] 0.9600465 0.8521607 0.8926037 0.9106642 0.9414073 0.9089535 0.8853734 #> [2,] 0.5688328 0.4729379 0.3001002 0.4016827 0.1537747 0.5020279 0.4499136 #> [3,] 0.7624066 0.6755266 0.4215342 0.7890541 0.8841856 0.8045081 0.8569334 #> [4,] 0.9538866 0.8506890 0.8853153 0.8871057 0.8586674 0.8929029 0.8870742 #> [5,] 0.3328679 0.3322537 0.1985050 0.2144293 0.1875264 0.2040099 0.2402087 #> [6,] 0.3710686 0.4288814 0.2510564 0.3075837 0.2703691 0.2781013 0.3354512 #>          [,64]     [,65]     [,66]     [,67]     [,68]     [,69]     [,70] #> [1,] 0.9481938 0.8850330 0.9556438 0.9683620 0.9391178 0.9453940 0.9215079 #> [2,] 0.5348796 0.3060510 0.2182378 0.2210106 0.4083197 0.2448369 0.2961248 #> [3,] 0.9198466 0.5835751 0.7399269 0.9510812 0.8669052 0.8688881 0.4998061 #> [4,] 0.9548685 0.8812970 0.9336504 0.9137324 0.9031129 0.9204905 0.9100478 #> [5,] 0.4597274 0.3237551 0.3396684 0.6053935 0.4874131 0.2033742 0.2378558 #> [6,] 0.5687552 0.3858880 0.4026785 0.8022279 0.5214094 0.3033622 0.2780328 #>          [,71]     [,72]     [,73]     [,74]     [,75]     [,76]     [,77] #> [1,] 0.9409108 0.9661708 0.9187898 0.9287160 0.8879047 0.9775874 0.9168241 #> [2,] 0.7454907 0.2111699 0.2933510 0.4803532 0.4832171 0.6699810 0.5163130 #> [3,] 0.9077010 0.8264248 0.7616756 0.8811049 0.8037181 0.9779914 0.9228643 #> [4,] 0.9383748 0.9461816 0.9181569 0.8951531 0.8749433 0.9733046 0.9150469 #> [5,] 0.2123416 0.3725571 0.2050060 0.3979848 0.6797451 0.4419641 0.4686999 #> [6,] 0.3170862 0.4603849 0.3051820 0.5585999 0.7279696 0.7042311 0.5971009 #>          [,78]     [,79]     [,80]     [,81]     [,82]     [,83]     [,84] #> [1,] 0.9351302 0.8698130 0.9402508 0.8111083 0.9526307 0.8828102 0.9810709 #> [2,] 0.2396743 0.4100818 0.4741427 0.3400261 0.2218101 0.5301549 0.3473553 #> [3,] 0.8647755 0.7484900 0.7328091 0.9392014 0.7718468 0.7607266 0.8537209 #> [4,] 0.9101152 0.7677945 0.9277486 0.7051606 0.7774003 0.8659909 0.9622713 #> [5,] 0.5845103 0.2410921 0.2996497 0.3130289 0.3798798 0.4773449 0.5002476 #> [6,] 0.6481464 0.3067603 0.4777281 0.4501688 0.5275403 0.5603578 0.6087540 #>          [,85]     [,86]     [,87]     [,88]     [,89]     [,90]     [,91] #> [1,] 0.8657193 0.8995534 0.9215794 0.9600188 0.9045619 0.9046810 0.8893577 #> [2,] 0.6010352 0.3226545 0.6377823 0.4771104 0.6683922 0.4326079 0.4654164 #> [3,] 0.9179295 0.5719712 0.8503718 0.8487589 0.8106474 0.9233210 0.6198703 #> [4,] 0.7719000 0.8285682 0.9159162 0.9522802 0.8896397 0.8694526 0.8477860 #> [5,] 0.5625906 0.1841121 0.2121038 0.3935640 0.3867989 0.4154533 0.1787833 #> [6,] 0.6148680 0.2261076 0.3292657 0.5207915 0.5533180 0.6181487 0.2662456 #>          [,92]     [,93]     [,94]     [,95]     [,96]     [,97]     [,98] #> [1,] 0.8023447 0.9840714 0.9404006 0.9428312 0.8031698 0.9588745 0.9549073 #> [2,] 0.4213005 0.3849919 0.4458837 0.6083645 0.4852906 0.4212425 0.1467606 #> [3,] 0.7613711 0.9789656 0.7345420 0.8524568 0.7916721 0.8668333 0.7958136 #> [4,] 0.7752236 0.9611236 0.9378620 0.9409270 0.8136503 0.9450509 0.9473404 #> [5,] 0.4251285 0.5429007 0.2539474 0.3625701 0.3111445 0.2439964 0.1314282 #> [6,] 0.5789391 0.6271542 0.3475937 0.4458035 0.3531202 0.4242612 0.2507183 #>          [,99]    [,100] #> [1,] 0.8962047 0.9479006 #> [2,] 0.5397747 0.4720353 #> [3,] 0.7816517 0.7494190 #> [4,] 0.9024435 0.9472594 #> [5,] 0.3184492 0.4796191 #> [6,] 0.5475009 0.5638147"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"threshold-based-decision-making","dir":"Articles","previous_headings":"4. Posterior Probability of Treatment Success","what":"Threshold-Based Decision-Making","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Suppose define treatment success predicted probability ≥ 0.6. can compute posterior probability threshold met subject.","code":"# Compute posterior probabilities of success (p_hat ≥ 0.6) success_threshold <- 0.6 posterior_probs_success <- rowMeans(posterior_preds >= success_threshold) head(posterior_probs_success) #> [1] 1.00 0.14 0.91 1.00 0.04 0.17"},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"proportion-of-subjects-likely-to-achieve-success","dir":"Articles","previous_headings":"4. Posterior Probability of Treatment Success","what":"Proportion of Subjects Likely to Achieve Success","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Next, calculate posterior probability certain proportion subjects treatment group achieve success.","code":"# Define success proportion threshold prop_success_threshold <- 0.7  # Simulate posterior proportion of success posterior_success_proportion <- colMeans(posterior_preds >= success_threshold)  # Posterior probability that ≥ 70% of subjects achieve success posterior_prob_high_success <- mean(posterior_success_proportion >= prop_success_threshold) posterior_prob_high_success #> [1] 0.16"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"posterior-predictive-distribution","dir":"Articles","previous_headings":"5. Visualizing Posterior Insights","what":"Posterior Predictive Distribution","title":"Using Bayesian Neural Networks in Clinical Trials","text":"","code":"library(ggplot2)  # Plot posterior probabilities of success for individual subjects ggplot(data.frame(Subject = seq_len(nrow(test_data)), Prob = posterior_probs_success), aes(x = Subject, y = Prob)) +   geom_bar(stat = \"identity\", fill = \"blue\") +   geom_hline(yintercept = success_threshold, color = \"red\", linetype = \"dashed\") +   labs(     title = \"Posterior Probability of Treatment Success\",     x = \"Subject\",     y = \"Posterior Probability\"   )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"distribution-of-success-proportion","dir":"Articles","previous_headings":"5. Visualizing Posterior Insights","what":"Distribution of Success Proportion","title":"Using Bayesian Neural Networks in Clinical Trials","text":"","code":"# Histogram of posterior success proportions ggplot(data.frame(SuccessProp = posterior_success_proportion), aes(x = SuccessProp)) +   geom_histogram(fill = \"green\", bins = 20) +   geom_vline(xintercept = prop_success_threshold, color = \"red\", linetype = \"dashed\") +   labs(     title = \"Posterior Distribution of Success Proportion\",     x = \"Proportion of Subjects Achieving Success\",     y = \"Frequency\"   )"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"bayesian-probability-threshold","dir":"Articles","previous_headings":"6. Clinical Trial Decision-Making","what":"Bayesian Probability Threshold","title":"Using Bayesian Neural Networks in Clinical Trials","text":"posterior probability can guide decision-making. example: - posterior_prob_high_success > 0.9, consider treatment effective. - posterior_prob_high_success < 0.1, consider treatment ineffective. - Otherwise, collect data refine model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"7. Conclusion","title":"Using Bayesian Neural Networks in Clinical Trials","text":"bnns package empowers clinical trial analysts leverage Bayesian Neural Networks predictive modeling decision-making. utilizing posterior distributions, can: - Quantify uncertainty predictions. - Make informed decisions treatment efficacy. - Evaluate trial outcomes based predefined success criteria. probabilistic framework particularly valuable scenarios uncertainty plays critical role decision-making, early-phase clinical trials. ```","code":""},{"path":"https://swarnendu-stat.github.io/bnns/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Swarnendu Chatterjee. Author, maintainer, copyright holder.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chatterjee S (2025). bnns: Bayesian Neural Network 'Stan'. R package version 0.1.1, https://swarnendu-stat.github.io/bnns/, https://github.com/swarnendu-stat/bnns.","code":"@Manual{,   title = {bnns: Bayesian Neural Network with 'Stan'},   author = {Swarnendu Chatterjee},   year = {2025},   note = {R package version 0.1.1, https://swarnendu-stat.github.io/bnns/},   url = {https://github.com/swarnendu-stat/bnns}, }"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"bnns-","dir":"","previous_headings":"","what":"Bayesian Neural Network with Stan","title":"Bayesian Neural Network with Stan","text":"bnns package provides tools fit Bayesian Neural Networks (BNNs) regression classification problems. designed flexible, supporting various network architectures, activation functions, output types, making suitable simple complex data analysis tasks.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Bayesian Neural Network with Stan","text":"Support multi-layer neural networks customizable architecture. Choice activation functions (e.g., sigmoid, ReLU, tanh). Outputs regression (continuous response) classification (binary multiclass). Choice prior distributions weights, biases sigma (regression). Bayesian inference, providing posterior distributions predictions parameters. Applications domains clinical trials, predictive modeling, .","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"installation-stable-cran-version","dir":"","previous_headings":"","what":"Installation (stable CRAN version)","title":"Bayesian Neural Network with Stan","text":"install bnns package CRAN, use following:","code":"install.packages(\"bnns\")"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"installation-development-version","dir":"","previous_headings":"","what":"Installation (development version)","title":"Bayesian Neural Network with Stan","text":"install bnns package GitHub, use following:","code":"# Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") }  # Install bnns devtools::install_github(\"swarnendu-stat/bnns\")"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_1-iris-data","dir":"","previous_headings":"Getting Started","what":"1. Iris Data","title":"Bayesian Neural Network with Stan","text":"use iris data regression:","code":"head(iris) #>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species #> 1          5.1         3.5          1.4         0.2  setosa #> 2          4.9         3.0          1.4         0.2  setosa #> 3          4.7         3.2          1.3         0.2  setosa #> 4          4.6         3.1          1.5         0.2  setosa #> 5          5.0         3.6          1.4         0.2  setosa #> 6          5.4         3.9          1.7         0.4  setosa"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_2-fit-a-bnn-model","dir":"","previous_headings":"Getting Started","what":"2. Fit a BNN Model","title":"Bayesian Neural Network with Stan","text":"fit Bayesian Neural Network:","code":"library(bnns)  iris_bnn <- bnns(Sepal.Length ~ -1 + ., data = iris, L = 1, act_fn = 3, nodes = 4, out_act_fn = 1, chains = 1)"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_3-model-summary","dir":"","previous_headings":"Getting Started","what":"3. Model Summary","title":"Bayesian Neural Network with Stan","text":"Summarize fitted model:","code":"summary(iris_bnn) #> Call: #> bnns.default(formula = Sepal.Length ~ -1 + ., data = iris, L = 1,  #>     nodes = 4, act_fn = 3, out_act_fn = 1, chains = 1) #>  #> Data Summary: #> Number of observations: 150  #> Number of features: 6  #>  #> Network Architecture: #> Number of hidden layers: 1  #> Nodes per layer: 4  #> Activation functions: 3  #> Output activation function: 1  #>  #> Posterior Summary (Key Parameters): #>                mean      se_mean         sd       2.5%         25%        50% #> w_out[1]  0.8345667 0.0728930420 0.65030179 -0.4150176  0.38054488  0.7769148 #> w_out[2] -0.3719132 0.4067431773 0.96605220 -1.7062097 -1.03225732 -0.7365945 #> w_out[3]  0.4783495 0.1965466796 0.86504113 -1.2350476  0.02944919  0.5634587 #> w_out[4]  0.4537029 0.3334670001 0.89069977 -1.3791675  0.09313077  0.5518418 #> b_out     2.2082591 0.0614548175 1.18859472 -0.1036760  1.38416657  2.2072194 #> sigma     0.3015085 0.0004831093 0.01804107  0.2693205  0.28895030  0.3013415 #>                75%     97.5%       n_eff      Rhat #> w_out[1] 1.2478028 2.1730066   79.589862 1.0254227 #> w_out[2] 0.4680286 1.7548944    5.641059 1.3136052 #> w_out[3] 1.0454306 2.0448172   19.370556 1.1335888 #> w_out[4] 1.0281249 2.0733860    7.134392 1.1997484 #> b_out    3.1573563 4.2214829  374.072451 1.0016806 #> sigma    0.3128869 0.3386066 1394.549362 0.9988699 #>  #> Model Fit Information: #> Iterations: 1000  #> Warmup: 200  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> RMSE (training): 0.2821305  #> MAE (training): 0.2234606  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values."},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_4-predictions","dir":"","previous_headings":"Getting Started","what":"4. Predictions","title":"Bayesian Neural Network with Stan","text":"Make predictions using trained model:","code":"pred <- predict(iris_bnn)"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_5-visualization","dir":"","previous_headings":"Getting Started","what":"5. Visualization","title":"Bayesian Neural Network with Stan","text":"Visualize true vs predicted values regression:","code":"plot(iris$Sepal.Length, rowMeans(pred), main = \"True vs Predicted\", xlab = \"True Values\", ylab = \"Predicted Values\") abline(0, 1, col = \"red\")"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"regression-example-with-custom-priors","dir":"","previous_headings":"Applications","what":"Regression Example (with custom priors)","title":"Bayesian Neural Network with Stan","text":"Use bnns regression analysis model continuous outcomes, predicting patient biomarkers clinical trials.","code":"model <- bnns(Sepal.Length ~ -1 + .,   data = iris, L = 1, act_fn = 3, nodes = 4,   out_act_fn = 1, chains = 1,   prior_weights = list(dist = \"uniform\", params = list(alpha = -1, beta = 1)),   prior_bias = list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)),   prior_sigma = list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) )"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"classification-example","dir":"","previous_headings":"Applications","what":"Classification Example","title":"Bayesian Neural Network with Stan","text":"binary multiclass classification, set out_act_fn 2 (binary) 3 (multiclass). example:","code":"# Simulate binary classification data df <- data.frame(   x1 = runif(10), x2 = runif(10),   y = sample(0:1, 10, replace = TRUE) )  # Fit a binary classification BNN model <- bnns(y ~ -1 + x1 + x2,   data = df, L = 2, nodes = c(16, 8),   act_fn = c(3, 2), out_act_fn = 2, iter = 1e2,   warmup = 5e1, chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"clinical-trial-applications","dir":"","previous_headings":"Applications","what":"Clinical Trial Applications","title":"Bayesian Neural Network with Stan","text":"Explore posterior probabilities estimate treatment effects success probabilities clinical trials. example, calculate posterior probability achieving clinically meaningful outcome given population.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Bayesian Neural Network with Stan","text":"Detailed vignettes available guide various applications package. See help(bnns) information bnns function arguments.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Bayesian Neural Network with Stan","text":"Contributions welcome! Please raise issues submit pull requests GitHub.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Bayesian Neural Network with Stan","text":"package licensed Apache License. See LICENSE details.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"Fits Bayesian Neural Network (BNN) model using formula interface. function parses formula data create input feature matrix target vector, fits model using bnns.default.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"","code":"# Default S3 method bnns(   formula,   data,   L = 1,   nodes = rep(2, L),   act_fn = rep(2, L),   out_act_fn = 1,   iter = 1000,   warmup = 200,   thin = 1,   chains = 2,   cores = 2,   seed = 123,   prior_weights = NULL,   prior_bias = NULL,   prior_sigma = NULL,   verbose = FALSE,   refresh = max(iter/10, 1),   normalize = TRUE,   ... )"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"formula symbolic description model fitted. formula specify response variable predictors (e.g., y ~ x1 + x2). data data frame containing variables model. L integer specifying number hidden layers neural network. Default 1. nodes integer vector specifying number nodes hidden layer. single value provided, applied layers. Default 16. act_fn integer vector specifying activation function(s) hidden layers. Options : 1 tanh 2 sigmoid (default) 3 softplus 4 ReLU 5 linear out_act_fn integer specifying activation function output layer. Options : 1 linear (default) 2 sigmoid 3 softmax iter integer specifying total number iterations Stan sampler. Default 1e3. warmup integer specifying number warmup iterations Stan sampler. Default 2e2. thin integer specifying thinning interval Stan samples. Default 1. chains integer specifying number Markov chains. Default 2. cores integer specifying number CPU cores use parallel sampling. Default 2. seed integer specifying random seed reproducibility. Default 123. prior_weights list specifying prior distribution weights neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_weights NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_bias list specifying prior distribution biases neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_bias NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_sigma list specifying prior distribution sigma parameter regression models (out_act_fn = 1). allows setting priors standard deviation residuals. list must include two components: dist: character string specifying distribution type. Supported values \"half-normal\" \"inverse-gamma\". params: named list specifying parameters chosen distribution: \"half-normal\": Provide sd (standard deviation half-normal distribution). \"inverse-gamma\": Provide shape (shape parameter) scale (scale parameter). prior_sigma NULL, default prior half-normal(0, 1) distribution. example: list(dist = \"half_normal\", params = list(mean = 0, sd = 1)) list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) verbose TRUE FALSE: flag indicating whether print intermediate output Stan console, might helpful model debugging. refresh refresh (integer) can used control often progress sampling reported (.e. show progress every refresh iterations). default, refresh = max(iter/10, 1). progress indicator turned refresh <= 0. normalize Logical. TRUE (default), input predictors normalized zero mean unit variance training. Normalization ensures stable efficient Bayesian sampling standardizing input scale, particularly beneficial neural network training. FALSE, normalization applied, assumed input data already pre-processed appropriately. ... Currently use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"object class \"bnns\" containing fitted model associated information, including: fit: fitted Stan model object. data: list containing processed training data. call: matched function call. formula: formula used model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"function uses provided formula data generate design matrix predictors response vector. calls helper function bnns_train fit Bayesian Neural Network model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"","code":"# \\donttest{ # Example usage: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 3,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.006 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:  # }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Function for Fitting Bayesian Neural Network Models — bnns","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"generic function fitting Bayesian Neural Network (BNN) models. dispatches methods based class input data.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"","code":"bnns(   formula,   data,   L = 1,   nodes = rep(2, L),   act_fn = rep(2, L),   out_act_fn = 1,   iter = 1000,   warmup = 200,   thin = 1,   chains = 2,   cores = 2,   seed = 123,   prior_weights = NULL,   prior_bias = NULL,   prior_sigma = NULL,   verbose = FALSE,   refresh = max(iter/10, 1),   normalize = TRUE,   ... )"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"formula symbolic description model fitted. formula specify response variable predictors (e.g., y ~ x1 + x2). y must continuous regression (out_act_fn = 1), numeric 0/1 binary classification (out_act_fn = 2), factor least 3 levels multi-classification (out_act_fn = 3). data data frame containing variables model. L integer specifying number hidden layers neural network. Default 1. nodes integer vector specifying number nodes hidden layer. single value provided, applied layers. Default 16. act_fn integer vector specifying activation function(s) hidden layers. Options : 1 tanh 2 sigmoid (default) 3 softplus 4 ReLU 5 linear out_act_fn integer specifying activation function output layer. Options : 1 linear (default) 2 sigmoid 3 softmax iter integer specifying total number iterations Stan sampler. Default 1e3. warmup integer specifying number warmup iterations Stan sampler. Default 2e2. thin integer specifying thinning interval Stan samples. Default 1. chains integer specifying number Markov chains. Default 2. cores integer specifying number CPU cores use parallel sampling. Default 2. seed integer specifying random seed reproducibility. Default 123. prior_weights list specifying prior distribution weights neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_weights NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_bias list specifying prior distribution biases neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_bias NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_sigma list specifying prior distribution sigma parameter regression models (out_act_fn = 1). allows setting priors standard deviation residuals. list must include two components: dist: character string specifying distribution type. Supported values \"half-normal\" \"inverse-gamma\". params: named list specifying parameters chosen distribution: \"half-normal\": Provide sd (standard deviation half-normal distribution). \"inverse-gamma\": Provide shape (shape parameter) scale (scale parameter). prior_sigma NULL, default prior half-normal(0, 1) distribution. example: list(dist = \"half_normal\", params = list(mean = 0, sd = 1)) list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) verbose TRUE FALSE: flag indicating whether print intermediate output Stan console, might helpful model debugging. refresh refresh (integer) can used control often progress sampling reported (.e. show progress every refresh iterations). default, refresh = max(iter/10, 1). progress indicator turned refresh <= 0. normalize Logical. TRUE (default), input predictors normalized zero mean unit variance training. Normalization ensures stable efficient Bayesian sampling standardizing input scale, particularly beneficial neural network training. FALSE, normalization applied, assumed input data already pre-processed appropriately. ... Currently use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"result method dispatched class input data. Typically, object class \"bnns\" containing fitted model associated information.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"function serves generic interface different methods fitting Bayesian Neural Networks. specific method dispatched depends class input arguments, allowing flexibility types inputs supported.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"Bishop, C.M., 1995. Neural networks pattern recognition. Oxford university press. Carpenter, B., Gelman, ., Hoffman, M.D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M.., Guo, J., Li, P. Riddell, ., 2017. Stan: probabilistic programming language. Journal statistical software, 76. Neal, R.M., 2012. Bayesian learning neural networks (Vol. 118). Springer Science & Business Media.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"","code":"# \\donttest{ # Example usage with formula interface: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 1,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.006 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:  # } # See the documentation for bnns.default for more details on the default implementation."},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function for training the BNN — bnns_train","title":"Internal function for training the BNN — bnns_train","text":"function performs actual fitting Bayesian Neural Network. called exported bnns methods intended direct use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function for training the BNN — bnns_train","text":"","code":"bnns_train(   train_x,   train_y,   L = 1,   nodes = rep(2, L),   act_fn = rep(2, L),   out_act_fn = 1,   iter = 1000,   warmup = 200,   thin = 1,   chains = 2,   cores = 2,   seed = 123,   prior_weights = NULL,   prior_bias = NULL,   prior_sigma = NULL,   verbose = FALSE,   refresh = max(iter/10, 1),   normalize = TRUE,   ... )"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function for training the BNN — bnns_train","text":"train_x numeric matrix representing input features (predictors) training. Rows correspond observations, columns correspond features. train_y numeric vector representing target values training. length must match number rows train_x. L integer specifying number hidden layers neural network. Default 1. nodes integer vector specifying number nodes hidden layer. single value provided, applied layers. Default 16. act_fn integer vector specifying activation function(s) hidden layers. Options : 1 tanh 2 sigmoid (default) 3 softplus 4 ReLU 5 linear out_act_fn integer specifying activation function output layer. Options : 1 linear (default) 2 sigmoid 3 softmax iter integer specifying total number iterations Stan sampler. Default 1e3. warmup integer specifying number warmup iterations Stan sampler. Default 2e2. thin integer specifying thinning interval Stan samples. Default 1. chains integer specifying number Markov chains. Default 2. cores integer specifying number CPU cores use parallel sampling. Default 2. seed integer specifying random seed reproducibility. Default 123. prior_weights list specifying prior distribution weights neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_weights NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_bias list specifying prior distribution biases neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_bias NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_sigma list specifying prior distribution sigma parameter regression models (out_act_fn = 1). allows setting priors standard deviation residuals. list must include two components: dist: character string specifying distribution type. Supported values \"half-normal\" \"inverse-gamma\". params: named list specifying parameters chosen distribution: \"half-normal\": Provide sd (standard deviation half-normal distribution). \"inverse-gamma\": Provide shape (shape parameter) scale (scale parameter). prior_sigma NULL, default prior half-normal(0, 1) distribution. example: list(dist = \"half_normal\", params = list(mean = 0, sd = 1)) list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) verbose TRUE FALSE: flag indicating whether print intermediate output Stan console, might helpful model debugging. refresh refresh (integer) can used control often progress sampling reported (.e. show progress every refresh iterations). default, refresh = max(iter/10, 1). progress indicator turned refresh <= 0. normalize Logical. TRUE (default), input predictors normalized zero mean unit variance training. Normalization ensures stable efficient Bayesian sampling standardizing input scale, particularly beneficial neural network training. FALSE, normalization applied, assumed input data already pre-processed appropriately. ... Currently use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function for training the BNN — bnns_train","text":"object class \"bnns\" containing following components: fit fitted Stan model object. call matched call. data list containing Stan data used model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function for training the BNN — bnns_train","text":"function uses generate_stan_code function dynamically generate Stan code based specified number layers nodes. Stan used fit Bayesian Neural Network.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal function for training the BNN — bnns_train","text":"","code":"# \\donttest{ # Example usage: train_x <- matrix(runif(20), nrow = 10, ncol = 2) train_y <- rnorm(10) model <- bnns::bnns_train(train_x, train_y,   L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.004 seconds (Warm-up) #> Chain 1:                0.003 seconds (Sampling) #> Chain 1:                0.007 seconds (Total) #> Chain 1:   # Access Stan model fit model$fit #> Inference for Stan model: anon_model. #> 1 chains, each with iter=100; warmup=50; thin=1;  #> post-warmup draws per chain=50, total post-warmup draws=50. #>  #>            mean se_mean   sd   2.5%    25%    50%    75% 97.5% n_eff Rhat #> w1[1,1]   -0.14    0.17 1.01  -2.14  -0.78  -0.25   0.65  1.61    35 1.04 #> w1[1,2]    0.06    0.23 1.15  -1.77  -0.83   0.19   0.76  2.39    25 0.99 #> w1[2,1]   -0.14    0.13 1.23  -2.66  -1.00  -0.16   1.02  1.83    85 0.99 #> w1[2,2]   -0.05    0.19 1.07  -2.29  -0.50   0.16   0.43  1.96    32 0.99 #> b1[1]     -0.05    0.12 0.91  -1.85  -0.48   0.09   0.48  1.49    55 0.98 #> b1[2]      0.05    0.19 1.10  -2.11  -0.68   0.23   0.74  2.12    33 1.07 #> w_out[1]  -0.02    0.11 0.88  -1.79  -0.48  -0.02   0.44  1.63    61 1.02 #> w_out[2]   0.21    0.14 0.98  -1.49  -0.61   0.41   0.92  1.78    49 1.02 #> b_out     -0.11    0.11 0.76  -1.44  -0.68  -0.18   0.54  1.09    46 1.01 #> sigma      1.13    0.03 0.27   0.74   0.91   1.07   1.25  1.71    69 0.98 #> lp__     -11.61    0.30 1.87 -15.16 -13.02 -11.63 -10.15 -8.71    39 1.00 #>  #> Samples were drawn using NUTS(diag_e) at Fri Jan 10 04:41:06 2025. #> For each parameter, n_eff is a crude measure of effective sample size, #> and Rhat is the potential scale reduction factor on split chains (at  #> convergence, Rhat=1). # }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","title":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","text":"function serves wrapper generate Stan code Bayesian neural networks tailored different types response variables. Based specified output activation function (out_act_fn), delegates code generation appropriate function continuous, binary, categorical response models.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","text":"","code":"generate_stan_code(num_layers, nodes, out_act_fn = 1)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","text":"num_layers integer specifying number hidden layers neural network. nodes vector integers, element specifies number nodes corresponding hidden layer. length vector must match num_layers. out_act_fn integer specifying output activation function, determining type response variable. Supported values : 1: Continuous response (identity function output layer). 2: Binary response (sigmoid function output layer). 3: Categorical response (softmax function output layer).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","text":"character string containing Stan code specified Bayesian neural network model. Stan model includes data, parameters, transformed parameters, model blocks, adjusted based specified response type.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","text":"function dynamically calls one following functions based value out_act_fn: Continuous response: Calls generate_stan_code_cont. Binary response: Calls generate_stan_code_bin. Categorical response: Calls generate_stan_code_cat. unsupported value provided out_act_fn, function throws error. generated Stan code adapted response type, including appropriate likelihood functions transformations.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal function to generate Stan Code Based on Output Activation Function — generate_stan_code","text":"","code":"# Generate Stan code for a continuous response model stan_code <- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 1) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   vector[n] y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #>   real<lower=0> sigma; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn[1] == 4) a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   else a1 = z1; #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else if (act_fn[2] == 4) a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   else a2 = z2; #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   sigma ~ PRIOR_SIGMA; #>   y ~ normal(y_hat, sigma); #> }  # Generate Stan code for a binary response model stan_code <- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 2) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=0, upper=1> y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn[1] == 4) a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   else a1 = z1; #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else if (act_fn[2] == 4) a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   else a2 = z2; #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   y ~ bernoulli_logit(y_hat); #> }  # Generate Stan code for a categorical response model stan_code <- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 3) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=1> y; #>   int<lower=1> act_fn[L]; #>   int<lower=2> K; // Number of categories #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   matrix[nodes[L], K] w_out; #>   vector[K] b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   matrix[n, K] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn[1] == 4) a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   else a1 = z1; #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else if (act_fn[2] == 4) a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   else a2 = z2; #>   y_hat = a2 * w_out + rep_matrix(b_out', n); #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   to_vector(w_out) ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   for (i in 1:n) y[i] ~ categorical_logit(y_hat[i]'); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","title":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"function generates Stan code Bayesian neural network model designed predict binary response variables. Stan code dynamically constructed based specified number hidden layers nodes per layer. supports various activation functions hidden layers, including tanh, sigmoid, softplus relu. model uses Bernoulli likelihood binary outcomes.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"","code":"generate_stan_code_bin(num_layers, nodes)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"num_layers integer specifying number hidden layers neural network. nodes vector integers, element specifies number nodes corresponding hidden layer. length vector must match num_layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"character string containing Stan code specified Bayesian neural network model. Stan model includes data, parameters, transformed parameters, model blocks. code adjusted based whether network one multiple hidden layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"generated Stan code models binary response variable using neural network. hidden layers apply specified activation functions, output layer applies logistic function predict probability binary outcome. one hidden layer: function simplifies Stan code structure. multiple hidden layers: code dynamically includes additional layers based input arguments. Supported activation functions hidden layers: 1: Tanh 2: Sigmoid 3: Softplus 4: ReLU 5: linear output layer uses logistic transformation (inv_logit) constrain predictions 0 1, aligns Bernoulli likelihood.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal function to generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"","code":"# Generate Stan code for a single hidden layer with 10 nodes stan_code <- generate_stan_code_bin(1, c(10)) cat(stan_code) #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes; #>   matrix[n, m] X; #>   array[n] int<lower=0, upper=1> y; #>   int<lower=1> act_fn; #> } #>  #> parameters { #>   matrix[m, nodes] w1; #>   vector[nodes] b1; #>   vector[nodes] w_out; #>   real b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes] z1; #>   matrix[n, nodes] a1; #>   vector[n] y_hat; #>  #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn == 1) a1 = tanh(z1); #>   else if (act_fn == 2) a1 = inv_logit(z1); #>   else if (act_fn == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn == 4) a1 = fmax(rep_matrix(0, n, nodes), z1); #>   else a1 = z1; #>  #>   y_hat = a1 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   y ~ bernoulli_logit(y_hat); #> }  # Generate Stan code for two hidden layers with 8 and 4 nodes stan_code <- generate_stan_code_bin(2, c(8, 4)) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=0, upper=1> y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn[1] == 4) a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   else a1 = z1; #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else if (act_fn[2] == 4) a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   else a2 = z2; #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   y ~ bernoulli_logit(y_hat); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","title":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"function generates Stan code modeling categorical response using neural networks multiple layers. generated code supports customizable activation functions layer softmax-based prediction categorical output.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"","code":"generate_stan_code_cat(num_layers, nodes)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"num_layers Integer. Number layers neural network. nodes Integer vector. Number nodes layer. length vector must match num_layers, values must positive.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"string containing Stan code specified neural network architecture categorical response model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"Stan code includes following components: Data Block: Defines inputs, response variable, layer configurations, activation functions. Parameters Block: Declares weights biases layers output layer. Transformed Parameters Block: Computes intermediate outputs (z ) layer calculates final predictions (y_hat) using softmax function. Model Block: Specifies priors parameters models categorical response using categorical_logit. Supported activation functions hidden layers: 1: Tanh 2: Sigmoid 3: Softplus 4: ReLU 5: linear categorical response (y) assumed take integer values 1 K, K total number categories.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal function to generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"","code":"# Generate Stan code for a neural network with 3 layers num_layers <- 3 nodes <- c(10, 8, 6) # 10 nodes in the first layer, 8 in the second, 6 in the third stan_code <- generate_stan_code_cat(num_layers, nodes) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=1> y; #>   int<lower=1> act_fn[L]; #>   int<lower=2> K; // Number of categories #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   matrix[nodes[2], nodes[3]] w3; #>   vector[nodes[3]] b3; #>   matrix[nodes[L], K] w_out; #>   vector[K] b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   matrix[n, nodes[3]] z3; #>   matrix[n, nodes[3]] a3; #>   matrix[n, K] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn[1] == 4) a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   else a1 = z1; #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else if (act_fn[2] == 4) a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   else a2 = z2; #>   z3 = a2 * w3 + rep_matrix(b3', n); #>   if (act_fn[3] == 1) a3 = tanh(z3); #>   else if (act_fn[3] == 2) a3 = inv_logit(z3); #>   else if (act_fn[3] == 3) a3 = log(1 + exp(z3)); #>   else if (act_fn[3] == 4) a3 = fmax(rep_matrix(0, n, nodes[3]), z3); #>   else a3 = z3; #>   y_hat = a3 * w_out + rep_matrix(b_out', n); #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   to_vector(w3) ~ PRIOR_WEIGHT; #>   b3 ~ PRIOR_BIAS; #>   to_vector(w_out) ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   for (i in 1:n) y[i] ~ categorical_logit(y_hat[i]'); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","title":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"function generates Stan code Bayesian neural network model designed predict continuous response variables. Stan code dynamically constructed based specified number hidden layers nodes per layer. supports various activation functions hidden layers, including tanh, sigmoid, softplus relu.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"","code":"generate_stan_code_cont(num_layers, nodes)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"num_layers integer specifying number hidden layers neural network. nodes vector integers, element specifies number nodes corresponding hidden layer. length vector must match num_layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"character string containing Stan code specified Bayesian neural network model. Stan model includes data, parameters, transformed parameters, model blocks. code adjusted based whether network one multiple hidden layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"generated Stan code models continuous response variable using neural network. hidden layers apply specified activation functions, output layer performs linear transformation predict response. likelihood assumes normally distributed residuals. one hidden layer: function simplifies Stan code structure. multiple hidden layers: code dynamically includes additional layers based input arguments. Supported activation functions hidden layers: 1: Tanh 2: Sigmoid 3: Softplus 4: ReLU 5: linear","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal function to generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"","code":"# Generate Stan code for a single hidden layer with 10 nodes stan_code <- generate_stan_code_cont(1, c(10)) cat(stan_code) #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes; #>   matrix[n, m] X; #>   vector[n] y; #>   int<lower=1> act_fn; #> } #>  #> parameters { #>   matrix[m, nodes] w1; #>   vector[nodes] b1; #>   vector[nodes] w_out; #>   real b_out; #>   real<lower=0> sigma; #> } #>  #> transformed parameters { #>   matrix[n, nodes] z1; #>   matrix[n, nodes] a1; #>   vector[n] y_hat; #>  #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn == 1) a1 = tanh(z1); #>   else if (act_fn == 2) a1 = inv_logit(z1); #>   else if (act_fn == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn == 4) a1 = fmax(rep_matrix(0, n, nodes), z1); #>   else a1 = z1; #>  #>   y_hat = a1 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   sigma ~ PRIOR_SIGMA; #>   y ~ normal(y_hat, sigma); #> }  # Generate Stan code for two hidden layers with 8 and 4 nodes stan_code <- generate_stan_code_cont(2, c(8, 4)) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   vector[n] y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #>   real<lower=0> sigma; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else if (act_fn[1] == 4) a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   else a1 = z1; #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else if (act_fn[2] == 4) a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   else a2 = z2; #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   sigma ~ PRIOR_SIGMA; #>   y ~ normal(y_hat, sigma); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure Performance for Binary Classification Models — measure_bin","title":"Measure Performance for Binary Classification Models — measure_bin","text":"Evaluates performance binary classification model using confusion matrix accuracy.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure Performance for Binary Classification Models — measure_bin","text":"","code":"measure_bin(obs, pred, cut = 0.5)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure Performance for Binary Classification Models — measure_bin","text":"obs numeric integer vector observed binary class labels (0 1). pred numeric vector predicted probabilities positive class. cut numeric threshold (0 1) classify predictions binary labels.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure Performance for Binary Classification Models — measure_bin","text":"list containing: conf_mat confusion matrix comparing observed predicted class labels. accuracy proportion correct predictions. ROC ROC generated using pROC::roc AUC Area ROC curve.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure Performance for Binary Classification Models — measure_bin","text":"","code":"obs <- c(1, 0, 1, 1, 0) pred <- c(0.9, 0.4, 0.8, 0.7, 0.3) cut <- 0.5 measure_bin(obs, pred, cut) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> $conf_mat #>    pred_label #> obs 0 1 #>   0 2 0 #>   1 0 3 #>  #> $accuracy #> [1] 1 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 2 controls (obs 0) < 3 cases (obs 1). #> Area under the curve: 1 #>  #> $AUC #> [1] 1 #>  # Returns: list(conf_mat = <confusion matrix>, accuracy = 1, ROC = <ROC>, AUC = 1)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure Performance for Multi-Class Classification Models — measure_cat","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"Evaluates performance multi-class classification model using log loss multiclass AUC.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"","code":"measure_cat(obs, pred)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"obs factor vector observed class labels. level represents unique class. pred numeric matrix predicted probabilities, row corresponds observation, column corresponds class. number columns must match number levels obs.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"list containing: log_loss negative log-likelihood averaged across observations. ROC ROC generated using pROC::roc AUC multiclass Area Curve (AUC) computed pROC::multiclass.roc.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"log loss calculated : $$-\\frac{1}{N} \\sum_{=1}^N \\sum_{c=1}^C y_{ic} \\log(p_{ic})$$ \\(y_{ic}\\) 1 observation \\(\\) belongs class \\(c\\), \\(p_{ic}\\) predicted probability class. AUC computed using pROC::multiclass.roc function, provides overall measure model performance multiclass classification.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"","code":"library(pROC) #> Type 'citation(\"pROC\")' for a citation. #>  #> Attaching package: ‘pROC’ #> The following objects are masked from ‘package:stats’: #>  #>     cov, smooth, var obs <- factor(c(\"A\", \"B\", \"C\"), levels = LETTERS[1:3]) pred <- matrix(   c(     0.8, 0.1, 0.1,     0.2, 0.6, 0.2,     0.7, 0.2, 0.1   ),   nrow = 3, byrow = TRUE ) measure_cat(obs, pred) #> $log_loss #> [1] 1.012185 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 3 levels of obs: A, B, C. #> Multi-class area under the curve: 0.75 #>  #> $AUC #> [1] 0.75 #>  # Returns: list(log_loss = 1.012185, ROC = <ROC>, AUC = 0.75)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure Performance for Continuous Response Models — measure_cont","title":"Measure Performance for Continuous Response Models — measure_cont","text":"Evaluates performance continuous response model using RMSE MAE.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure Performance for Continuous Response Models — measure_cont","text":"","code":"measure_cont(obs, pred)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure Performance for Continuous Response Models — measure_cont","text":"obs numeric vector observed (true) values. pred numeric vector predicted values.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure Performance for Continuous Response Models — measure_cont","text":"list containing: rmse Root Mean Squared Error. mae Mean Absolute Error.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure Performance for Continuous Response Models — measure_cont","text":"","code":"obs <- c(3.2, 4.1, 5.6) pred <- c(3.0, 4.3, 5.5) measure_cont(obs, pred) #> $rmse #> [1] 0.1732051 #>  #> $mae #> [1] 0.1666667 #>  # Returns: list(rmse = 0.1732051, mae = 0.1666667)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for ","title":"Predict Method for ","text":"Generates predictions fitted Bayesian Neural Network (BNN) model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for ","text":"","code":"# S3 method for class 'bnns' predict(object, newdata = NULL, ...)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for ","text":"object object class \"bnns\", typically result call bnns.default. newdata matrix data frame new input data predictions required. NULL, predictions made training data used fit model. ... Additional arguments (currently used).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for ","text":"matrix/array predicted values(regression)/probabilities(classification) first dimension corresponds rows newdata training data newdata NULL. Second dimension corresponds number posterior samples. case out_act_fn = 3, third dimension corresponds class.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for ","text":"function uses posterior distribution Stan model bnns object compute predictions provided input data.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for ","text":"","code":"# \\donttest{ # Example usage: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.005 seconds (Warm-up) #> Chain 1:                0.004 seconds (Sampling) #> Chain 1:                0.009 seconds (Total) #> Chain 1:  new_data <- data.frame(x1 = runif(5), x2 = runif(5)) predictions <- predict(model, newdata = new_data) print(predictions) #>           [,1]      [,2]       [,3]       [,4]       [,5]        [,6]      [,7] #> [1,] 1.1152597 0.1144449 -0.3084498 0.06871980  0.0695996 -0.12246975 1.6241251 #> [2,] 1.1032639 0.3355451 -0.2521142 0.10418440  0.2680478  0.06836728 1.1462482 #> [3,] 1.0090720 0.1644375 -0.2994728 0.08076507  0.0168862  0.08185471 1.2066402 #> [4,] 0.5735679 0.1241659 -0.3213966 0.06401085 -0.4587794  0.20939731 0.7790752 #> [5,] 1.2060703 0.2246150 -0.2433527 0.09789082  0.3983745 -0.23381176 1.5603237 #>             [,8]         [,9]      [,10]     [,11]       [,12]     [,13] #> [1,]  0.07928892  0.060746068 0.04866919 0.2481263 -0.78045166 0.1272458 #> [2,]  0.02421534 -0.006812054 0.33619422 0.6151307  0.19278759 0.2816045 #> [3,]  0.20339837  0.117731625 0.16056879 0.3974301 -0.48824078 0.1966143 #> [4,]  0.58989967  0.365229474 0.23692727 0.5830997 -0.73066422 0.1214295 #> [5,] -0.17710427 -0.121686334 0.26817930 0.3580093 -0.03640404 0.2224840 #>            [,14]        [,15]      [,16]       [,17]       [,18]       [,19] #> [1,] -0.08845954  0.003910156  0.9271427 -0.18182054  0.01519526 -0.18180187 #> [2,]  0.20376753  0.095816674  1.2626205 -0.01386249  0.16143325 -0.13348192 #> [3,] -0.01573832  0.157016461  0.8588632 -0.17245981  0.28143632 -0.14961337 #> [4,] -0.18453541  0.674553629 -0.4623328 -0.20383779  0.72618113 -0.09657372 #> [5,]  0.18043438 -0.038946667  1.3074441  0.10643182 -0.09443345 -0.17565812 #>          [,20]      [,21]     [,22]     [,23]      [,24]       [,25] #> [1,] 0.5760405 -0.1261391 0.8801627 0.2606440 -0.0273912 -0.21396836 #> [2,] 0.3371327  0.3323385 0.7708056 0.3251964  0.3811506 -0.07312034 #> [3,] 0.4222104 -0.0943842 0.8137951 0.2779615  0.1352468 -0.35081931 #> [4,] 0.2790991 -0.5858628 0.7412925 0.2444881  0.1093305 -0.63985754 #> [5,] 0.5194725  0.4243195 0.8460873 0.3198303  0.2181193  0.26014020 #>            [,26]       [,27]     [,28]     [,29]     [,30]       [,31] #> [1,]  0.06359765 -0.13620735 0.5204135 0.3099146 0.5753895  0.04505463 #> [2,] -0.18967770  0.02621941 0.6207272 0.3175207 0.4443012  0.56127908 #> [3,] -0.15300365 -0.08358536 0.4555343 0.3245022 0.4958966  0.14080992 #> [4,] -0.36075493 -0.08446071 0.2613028 0.3457268 0.4265476 -0.14889234 #> [5,]  0.06149019 -0.06206935 0.6838519 0.2913464 0.5382515  0.47210962 #>          [,32]       [,33]       [,34]       [,35]     [,36]       [,37] #> [1,] 0.1098968 -0.15546137 -0.06885152 -0.31246968 0.3168428 0.192410325 #> [2,] 0.1785457 -0.10074354  0.08294440  0.33097937 0.2856643 0.133701884 #> [3,] 0.1879182  0.01008226 -0.12829834 -0.07824051 0.3170884 0.337058983 #> [4,] 0.5049853  0.52601421 -0.41803611 -0.08976834 0.3931654 0.627696093 #> [5,] 0.1159979 -0.32481535  0.12217087  0.01719852 0.2836795 0.003239519 #>           [,38]       [,39]        [,40]     [,41]      [,42]       [,43] #> [1,]  0.2797295 -0.05656098 -0.004787517 0.5436973  0.4838993  0.37931754 #> [2,]  0.6408875 -0.06400121  0.015063325 0.4199698  0.4403711  0.06833263 #> [3,]  0.2904487 -0.04831487  0.025122396 0.3926606  0.3292979  0.10506024 #> [4,] -0.1836910 -0.03308653  0.071429429 0.1597367 -0.1035795 -0.44060472 #> [5,]  0.7340162 -0.09091135 -0.024061798 0.6157188  0.5594030  0.41883954 #>          [,44]       [,45]     [,46]     [,47]     [,48]      [,49]     [,50] #> [1,] 0.8062099 -0.26510132 0.3105637 0.1420912 1.2593505 0.07766318 0.3325269 #> [2,] 1.0178422  0.19162009 0.1300183 0.2459813 0.8448126 0.12437387 0.4972103 #> [3,] 0.6139744 -0.11284985 0.3008559 0.1506734 0.8842861 0.08655892 0.3209692 #> [4,] 0.2102558  0.04254392 0.4915077 0.1061463 0.5225944 0.06784583 0.1016278 #> [5,] 1.2650271  0.02352946 0.0851295 0.2718672 1.2352531 0.12266658 0.5743117 # }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for ","title":"Print Method for ","text":"Displays summary fitted Bayesian Neural Network (BNN) model, including function call Stan fit details.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for ","text":"","code":"# S3 method for class 'bnns' print(x, ...)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for ","text":"x object class \"bnns\", typically result call bnns.default. ... Additional arguments (currently used).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for ","text":"function called side effects return value. prints following: function call used generate \"bnns\" object. summary Stan fit object stored x$fit.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Method for ","text":"","code":"# \\donttest{ # Example usage: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.006 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:  print(model) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = data, L = 1,  #>     nodes = 2, act_fn = 2, iter = 100, warmup = 50, chains = 1) #>  #> Stan fit: #> Inference for Stan model: anon_model. #> 1 chains, each with iter=100; warmup=50; thin=1;  #> post-warmup draws per chain=50, total post-warmup draws=50. #>  #>           mean se_mean   sd   2.5%   25%   50%   75% 97.5% n_eff Rhat #> w1[1,1]  -0.02    0.11 0.84  -1.62 -0.67  0.01  0.54  1.76    53 0.98 #> w1[1,2]   0.12    0.14 1.09  -1.55 -0.82 -0.08  0.95  2.05    58 0.99 #> w1[2,1]  -0.02    0.15 0.98  -1.78 -0.57 -0.27  0.82  1.48    43 0.99 #> w1[2,2]   0.09    0.10 0.83  -1.48 -0.52  0.26  0.59  1.57    66 1.03 #> b1[1]     0.04    0.10 0.87  -1.51 -0.47 -0.01  0.56  1.72    73 0.98 #> b1[2]     0.17    0.09 0.77  -1.18 -0.43  0.23  0.66  1.36    69 0.98 #> w_out[1]  0.11    0.11 0.88  -1.88 -0.28  0.14  0.85  1.51    60 0.99 #> w_out[2] -0.12    0.10 0.93  -1.57 -0.79 -0.14  0.39  1.83    85 0.98 #> b_out    -0.32    0.10 0.72  -1.63 -0.78 -0.33  0.09  1.46    52 0.99 #> sigma     0.97    0.03 0.27   0.59  0.81  0.91  1.09  1.61    59 1.01 #> lp__     -8.30    0.38 2.09 -13.03 -9.08 -7.89 -6.95 -5.15    30 0.98 #>  #> Samples were drawn using NUTS(diag_e) at Fri Jan 10 04:43:18 2025. #> For each parameter, n_eff is a crude measure of effective sample size, #> and Rhat is the potential scale reduction factor on split chains (at  #> convergence, Rhat=1). # }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":null,"dir":"Reference","previous_headings":"","what":"relu transformation — relu","title":"relu transformation — relu","text":"relu transformation","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"relu transformation — relu","text":"","code":"relu(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"relu transformation — relu","text":"x numeric vector matrix relu transformation going applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"relu transformation — relu","text":"numeric vector matrix relu transformation.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"relu transformation — relu","text":"","code":"relu(matrix(1:4, , nrow = 2)) #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4"},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"sigmoid transformation — sigmoid","title":"sigmoid transformation — sigmoid","text":"sigmoid transformation","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sigmoid transformation — sigmoid","text":"","code":"sigmoid(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"sigmoid transformation — sigmoid","text":"x numeric vector matrix sigmoid transformation going applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"sigmoid transformation — sigmoid","text":"numeric vector matrix sigmoid transformation.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"sigmoid transformation — sigmoid","text":"","code":"sigmoid(matrix(1:4, nrow = 2)) #>           [,1]      [,2] #> [1,] 0.7310586 0.9525741 #> [2,] 0.8807971 0.9820138"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Softmax Function to a 3D Array — softmax_3d","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"function applies softmax transformation along third dimension 3D array. softmax function converts raw scores probabilities sum 1 slice along third dimension.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"","code":"softmax_3d(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"x 3D array. input array softmax function applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"3D array dimensions x, values along third dimension transformed using softmax function.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"softmax transformation computed : $$\\text{softmax}(x_{ijk}) = \\frac{\\exp(x_{ijk})}{\\sum_{l} \\exp(x_{ijl})}$$ applied pair indices (, j) across third dimension (k). function processes input array slice--slice first two dimensions (, j), normalizing values along third dimension (k) slice.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"","code":"# Example: Apply softmax to a 3D array x <- array(runif(24), dim = c(2, 3, 4)) # Random 3D array (2x3x4) softmax_result <- softmax_3d(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":null,"dir":"Reference","previous_headings":"","what":"softplus transformation — softplus","title":"softplus transformation — softplus","text":"softplus transformation","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"softplus transformation — softplus","text":"","code":"softplus(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"softplus transformation — softplus","text":"x numeric vector matrix softplus transformation going applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"softplus transformation — softplus","text":"numeric vector matrix softplus transformation.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"softplus transformation — softplus","text":"","code":"softplus(matrix(1:4, nrow = 2)) #>          [,1]     [,2] #> [1,] 1.313262 3.048587 #> [2,] 2.126928 4.018150"},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"Provides comprehensive summary fitted Bayesian Neural Network (BNN) model, including details model call, data, network architecture, posterior distributions, model fitting information.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"","code":"# S3 method for class 'bnns' summary(object, ...)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"object object class bnns, representing fitted Bayesian Neural Network model. ... Additional arguments (currently unused).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"list (returned invisibly) containing following elements: \"Number observations\": number observations training data. \"Number features\": number features training data. \"Number hidden layers\": number hidden layers neural network. \"Nodes per layer\": comma-separated string representing number nodes hidden layer. \"Activation functions\": comma-separated string representing activation functions used hidden layer. \"Output activation function\": activation function used output layer. \"Stanfit Summary\": summary Stan model, including key parameter posterior distributions. \"Iterations\": total number iterations used sampling Bayesian model. \"Warmup\": number iterations used warmup Bayesian model. \"Thinning\": thinning interval used Bayesian model. \"Chains\": number Markov chains used Bayesian model. \"Performance\": Predictive performance metrics, vary based output activation function. function also prints summary console.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"function prints following information: Call: original function call used fit model. Data Summary: Number observations features training data. Network Architecture: Structure BNN including number hidden layers, nodes per layer, activation functions. Posterior Summary: Summarized posterior distributions key parameters (e.g., weights, biases, noise parameter). Model Fit Information: Bayesian sampling details, including number iterations, warmup period, thinning, chains. Notes: Remarks warnings, checks convergence diagnostics.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"","code":"# \\donttest{ # Fit a Bayesian Neural Network data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.005 seconds (Warm-up) #> Chain 1:                0.004 seconds (Sampling) #> Chain 1:                0.009 seconds (Total) #> Chain 1:   # Get a summary of the model summary(model) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = data, L = 1,  #>     nodes = 2, act_fn = 2, iter = 100, warmup = 50, chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 1  #> Nodes per layer: 2  #> Activation functions: 2  #> Output activation function: 1  #>  #> Posterior Summary (Key Parameters): #>                 mean    se_mean        sd       2.5%        25%         50% #> w_out[1] -0.05432372 0.09247250 0.8053585 -1.8116323 -0.5900625 -0.01134891 #> w_out[2] -0.20667669 0.11063755 0.8241266 -1.6868328 -0.7247308 -0.15475959 #> b_out     0.22370414 0.08898919 0.6674141 -0.7754468 -0.1774560  0.16644801 #> sigma     1.17390514 0.02727134 0.2513531  0.8313935  1.0085272  1.09219326 #>                75%    97.5%    n_eff      Rhat #> w_out[1] 0.5618538 1.316687 75.84960 0.9939791 #> w_out[2] 0.3165364 1.430570 55.48592 1.0217312 #> b_out    0.6322990 1.604595 56.24919 1.0572413 #> sigma    1.3415874 1.736803 84.94850 0.9825971 #>  #> Model Fit Information: #> Iterations: 100  #> Warmup: 50  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> RMSE (training): 1.1672  #> MAE (training): 1.016994  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values. # }"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/news/index.html","id":"bnns-010","dir":"Changelog","previous_headings":"","what":"bnns 0.1.0","title":"bnns 0.1.0","text":"Initial release bnns package. Implements Bayesian Neural Networks using ‘Stan’. Provides flexible model specification formula Bayesian estimation.","code":""}]
