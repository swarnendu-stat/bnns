[{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"Apache License","title":"Apache License","text":"Version 2.0, January 2004 <http://www.apache.org/licenses/>","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_1-definitions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"1. Definitions","title":"Apache License","text":"“License” shall mean terms conditions use, reproduction, distribution defined Sections 1 9 document. “Licensor” shall mean copyright owner entity authorized copyright owner granting License. “Legal Entity” shall mean union acting entity entities control, controlled , common control entity. purposes definition, “control” means () power, direct indirect, cause direction management entity, whether contract otherwise, (ii) ownership fifty percent (50%) outstanding shares, (iii) beneficial ownership entity. “” (“”) shall mean individual Legal Entity exercising permissions granted License. “Source” form shall mean preferred form making modifications, including limited software source code, documentation source, configuration files. “Object” form shall mean form resulting mechanical transformation translation Source form, including limited compiled object code, generated documentation, conversions media types. “Work” shall mean work authorship, whether Source Object form, made available License, indicated copyright notice included attached work (example provided Appendix ). “Derivative Works” shall mean work, whether Source Object form, based (derived ) Work editorial revisions, annotations, elaborations, modifications represent, whole, original work authorship. purposes License, Derivative Works shall include works remain separable , merely link (bind name) interfaces , Work Derivative Works thereof. “Contribution” shall mean work authorship, including original version Work modifications additions Work Derivative Works thereof, intentionally submitted Licensor inclusion Work copyright owner individual Legal Entity authorized submit behalf copyright owner. purposes definition, “submitted” means form electronic, verbal, written communication sent Licensor representatives, including limited communication electronic mailing lists, source code control systems, issue tracking systems managed , behalf , Licensor purpose discussing improving Work, excluding communication conspicuously marked otherwise designated writing copyright owner “Contribution.” “Contributor” shall mean Licensor individual Legal Entity behalf Contribution received Licensor subsequently incorporated within Work.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_2-grant-of-copyright-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"2. Grant of Copyright License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable copyright license reproduce, prepare Derivative Works , publicly display, publicly perform, sublicense, distribute Work Derivative Works Source Object form.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_3-grant-of-patent-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"3. Grant of Patent License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable (except stated section) patent license make, made, use, offer sell, sell, import, otherwise transfer Work, license applies patent claims licensable Contributor necessarily infringed Contribution(s) alone combination Contribution(s) Work Contribution(s) submitted. institute patent litigation entity (including cross-claim counterclaim lawsuit) alleging Work Contribution incorporated within Work constitutes direct contributory patent infringement, patent licenses granted License Work shall terminate date litigation filed.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_4-redistribution","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"4. Redistribution","title":"Apache License","text":"may reproduce distribute copies Work Derivative Works thereof medium, without modifications, Source Object form, provided meet following conditions: () must give recipients Work Derivative Works copy License; (b) must cause modified files carry prominent notices stating changed files; (c) must retain, Source form Derivative Works distribute, copyright, patent, trademark, attribution notices Source form Work, excluding notices pertain part Derivative Works; (d) Work includes “NOTICE” text file part distribution, Derivative Works distribute must include readable copy attribution notices contained within NOTICE file, excluding notices pertain part Derivative Works, least one following places: within NOTICE text file distributed part Derivative Works; within Source form documentation, provided along Derivative Works; , within display generated Derivative Works, wherever third-party notices normally appear. contents NOTICE file informational purposes modify License. may add attribution notices within Derivative Works distribute, alongside addendum NOTICE text Work, provided additional attribution notices construed modifying License. may add copyright statement modifications may provide additional different license terms conditions use, reproduction, distribution modifications, Derivative Works whole, provided use, reproduction, distribution Work otherwise complies conditions stated License.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_5-submission-of-contributions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"5. Submission of Contributions","title":"Apache License","text":"Unless explicitly state otherwise, Contribution intentionally submitted inclusion Work Licensor shall terms conditions License, without additional terms conditions. Notwithstanding , nothing herein shall supersede modify terms separate license agreement may executed Licensor regarding Contributions.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_6-trademarks","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"6. Trademarks","title":"Apache License","text":"License grant permission use trade names, trademarks, service marks, product names Licensor, except required reasonable customary use describing origin Work reproducing content NOTICE file.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_7-disclaimer-of-warranty","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"7. Disclaimer of Warranty","title":"Apache License","text":"Unless required applicable law agreed writing, Licensor provides Work (Contributor provides Contributions) “” BASIS, WITHOUT WARRANTIES CONDITIONS KIND, either express implied, including, without limitation, warranties conditions TITLE, NON-INFRINGEMENT, MERCHANTABILITY, FITNESS PARTICULAR PURPOSE. solely responsible determining appropriateness using redistributing Work assume risks associated exercise permissions License.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_8-limitation-of-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"8. Limitation of Liability","title":"Apache License","text":"event legal theory, whether tort (including negligence), contract, otherwise, unless required applicable law (deliberate grossly negligent acts) agreed writing, shall Contributor liable damages, including direct, indirect, special, incidental, consequential damages character arising result License use inability use Work (including limited damages loss goodwill, work stoppage, computer failure malfunction, commercial damages losses), even Contributor advised possibility damages.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"id_9-accepting-warranty-or-additional-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"9. Accepting Warranty or Additional Liability","title":"Apache License","text":"redistributing Work Derivative Works thereof, may choose offer, charge fee , acceptance support, warranty, indemnity, liability obligations /rights consistent License. However, accepting obligations, may act behalf sole responsibility, behalf Contributor, agree indemnify, defend, hold Contributor harmless liability incurred , claims asserted , Contributor reason accepting warranty additional liability. END TERMS CONDITIONS","code":""},{"path":"https://swarnendu-stat.github.io/bnns/LICENSE.html","id":"appendix-how-to-apply-the-apache-license-to-your-work","dir":"","previous_headings":"","what":"APPENDIX: How to apply the Apache License to your work","title":"Apache License","text":"apply Apache License work, attach following boilerplate notice, fields enclosed brackets [] replaced identifying information. (Don’t include brackets!) text enclosed appropriate comment syntax file format. also recommend file class name description purpose included “printed page” copyright notice easier identification within third-party archives.","code":"Copyright [yyyy] [name of copyright owner]  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"bnns","text":"bnns package provides efficient user-friendly implementation Bayesian Neural Networks (BNNs) regression, binary classification, multiclass classification problems. integrating Bayesian inference, bnns allows uncertainty quantification predictions robust parameter estimation. vignette covers: 1. Installing loading package 2. Preparing data 3. Fitting BNN model 4. Summarizing model 5. Making predictions 6. Model evaluation 7. Customizing prior","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"installation","dir":"Articles","previous_headings":"","what":"1. Installation","title":"bnns","text":"install package, use following commands: Load package R session:","code":"# Install from CRAN (if available) #  install.packages(\"bnns\")  # Or install the development version from GitHub # devtools::install_github(\"swarnendu-stat/bnns\") library(bnns)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"preparing-the-data","dir":"Articles","previous_headings":"","what":"2. Preparing the Data","title":"bnns","text":"bnns package expects data form matrices predictors vector responses. ’s example generating synthetic data: binary multiclass classification:","code":"# Generate training data set.seed(123) df <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) # Binary classification response df$y_bin <- sample(0:1, 10, replace = TRUE)  # Multiclass classification response df$y_cat <- factor(sample(letters[1:3], 10, replace = TRUE)) # 3 classes"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"fitting-a-bayesian-neural-network-model","dir":"Articles","previous_headings":"","what":"3. Fitting a Bayesian Neural Network Model","title":"bnns","text":"Fit Bayesian Neural Network using bnns() function. Specify network architecture using arguments like number layers (L), nodes per layer (nodes), activation functions (act_fn).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"regression-example","dir":"Articles","previous_headings":"3. Fitting a Bayesian Neural Network Model","what":"Regression Example","title":"bnns","text":"","code":"model_reg <- bnns(   y ~ -1 + x1 + x2,   data = df,   L = 2, # Number of hidden layers   nodes = c(16, 8), # Nodes per layer   act_fn = c(2, 3), # Activation functions: 2 = Sigmoid, 3 = ReLU   out_act_fn = 1, # Output activation function: 1 = Identity (for regression)   iter = 2e2,   warmup = 1e2,   chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"binary-classification-example","dir":"Articles","previous_headings":"3. Fitting a Bayesian Neural Network Model","what":"Binary Classification Example","title":"bnns","text":"","code":"model_bin <- bnns(   y_bin ~ -1 + x1 + x2,   data = df,   L = 1,   nodes = c(16),   act_fn = c(2),   out_act_fn = 2, # Output activation: 2 = Logistic sigmoid   iter = 2e2,   warmup = 1e2,   chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"multiclass-classification-example","dir":"Articles","previous_headings":"3. Fitting a Bayesian Neural Network Model","what":"Multiclass Classification Example","title":"bnns","text":"","code":"model_cat <- bnns(   y_cat ~ -1 + x1 + x2,   data = df,   L = 3,   nodes = c(32, 16, 8),   act_fn = c(3, 2, 2),   out_act_fn = 3, # Output activation: 3 = Softmax   iter = 2e2,   warmup = 1e2,   chains = 1 )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"summarizing-the-model","dir":"Articles","previous_headings":"","what":"4. Summarizing the Model","title":"bnns","text":"Use summary() function view details fitted model, including network architecture, posterior distributions, predictive performance.","code":"summary(model_reg) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = df, L = 2, nodes = c(16,  #>     8), act_fn = c(2, 3), out_act_fn = 1, iter = 200, warmup = 100,  #>     chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 2  #> Nodes per layer: 16, 8  #> Activation functions: 2, 3  #> Output activation function: 1  #>  #> Posterior Summary (Key Parameters): #>                 mean    se_mean        sd       2.5%        25%         50% #> w_out[1] -0.07970081 0.06846247 0.9682056 -2.1360113 -0.6963688 -0.16679456 #> w_out[2]  0.06456191 0.10749989 1.0291658 -2.1225425 -0.5663087  0.11947689 #> w_out[3]  0.11422267 0.06824486 0.8453540 -1.3298244 -0.5145013  0.14489377 #> w_out[4] -0.03942275 0.07463729 0.7954668 -1.4593665 -0.6167020 -0.00268951 #> w_out[5]  0.05517880 0.11031575 0.9477827 -1.6628032 -0.6993092  0.06904414 #> w_out[6] -0.10051502 0.08215889 0.8566549 -1.6301129 -0.6097178 -0.10033871 #> w_out[7]  0.01175058 0.08029138 0.9024368 -2.0663648 -0.4880417  0.06540799 #> w_out[8] -0.13108646 0.07697444 0.8361898 -1.5942451 -0.7825919 -0.12908555 #> b_out     0.01766977 0.10965748 1.0232441 -1.7807058 -0.6878054  0.10364959 #> sigma     0.82683004 0.02089225 0.2432927  0.4965386  0.6407224  0.79565441 #>                75%    97.5%     n_eff      Rhat #> w_out[1] 0.6512718 1.518476 200.00000 0.9912740 #> w_out[2] 0.7142186 1.789971  91.65469 0.9899925 #> w_out[3] 0.6141343 2.047458 153.43956 0.9899708 #> w_out[4] 0.4394828 1.363184 113.58801 0.9932006 #> w_out[5] 0.6542098 1.937073  73.81464 1.0061040 #> w_out[6] 0.3287825 1.773595 108.71831 0.9908565 #> w_out[7] 0.5665454 1.653195 126.32686 0.9920570 #> w_out[8] 0.4291506 1.343326 118.00941 0.9906425 #> b_out    0.6020728 1.700598  87.07270 0.9899495 #> sigma    0.9639360 1.298553 135.60875 0.9978727 #>  #> Model Fit Information: #> Iterations: 200  #> Warmup: 100  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> RMSE (training): 0.6187873  #> MAE (training): 0.5015489  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values. summary(model_bin) #> Call: #> bnns.default(formula = y_bin ~ -1 + x1 + x2, data = df, L = 1,  #>     nodes = c(16), act_fn = c(2), out_act_fn = 2, iter = 200,  #>     warmup = 100, chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 1  #> Nodes per layer: 16  #> Activation functions: 2  #> Output activation function: 2  #>  #> Posterior Summary (Key Parameters): #>                  mean    se_mean        sd      2.5%        25%         50% #> w_out[1]  -0.10110551 0.07690621 0.8918423 -1.695352 -0.8001027  0.01444756 #> w_out[2]  -0.08726693 0.06605833 0.9342059 -1.774685 -0.7822605 -0.17450266 #> w_out[3]  -0.18923401 0.06313568 0.8928733 -1.948382 -0.6916708 -0.06156762 #> w_out[4]  -0.13519437 0.07263513 0.9546914 -2.131777 -0.7984818 -0.10924737 #> w_out[5]  -0.05836183 0.06911558 0.8600123 -1.601996 -0.6339802 -0.04144955 #> w_out[6]  -0.11256831 0.07408610 1.0477356 -2.115622 -0.9079361 -0.05380241 #> w_out[7]  -0.11107491 0.07208329 0.8974309 -1.745066 -0.7319325 -0.08583247 #> w_out[8]  -0.05894826 0.07540536 1.0663929 -2.185835 -0.7446250 -0.01440285 #> w_out[9]  -0.11478543 0.07315091 1.0345101 -2.252466 -0.7495023 -0.08728692 #> w_out[10]  0.03207499 0.07732217 0.9144306 -1.517396 -0.6228746 -0.01176603 #> w_out[11] -0.12440063 0.06514516 0.9212917 -1.963137 -0.7124421 -0.06475202 #> w_out[12] -0.15843373 0.07875792 1.1138051 -2.430101 -0.6615781 -0.01753044 #> w_out[13] -0.14744805 0.06857286 0.9697667 -2.189953 -0.7890001 -0.11954830 #> w_out[14] -0.22538573 0.06788229 0.7980628 -1.696545 -0.7452381 -0.22185988 #> w_out[15] -0.15469604 0.06234291 0.8816618 -1.782075 -0.7271137 -0.11837500 #> w_out[16] -0.07055980 0.05682076 0.8035668 -1.604959 -0.5366324 -0.08063014 #> b_out     -0.14244196 0.05992138 0.8474163 -1.786832 -0.6901619 -0.04366264 #>                 75%    97.5%    n_eff      Rhat #> w_out[1]  0.5388349 1.504491 134.4787 0.9899641 #> w_out[2]  0.6312955 1.745942 200.0000 0.9919117 #> w_out[3]  0.3466554 1.456184 200.0000 0.9991987 #> w_out[4]  0.3672884 1.744327 172.7558 0.9900617 #> w_out[5]  0.4294623 1.613986 154.8308 0.9900073 #> w_out[6]  0.6569482 1.649661 200.0000 0.9925097 #> w_out[7]  0.5617465 1.533728 155.0004 0.9922798 #> w_out[8]  0.6694988 1.876662 200.0000 1.0106131 #> w_out[9]  0.5167634 1.866479 200.0000 0.9902438 #> w_out[10] 0.6673431 1.826614 139.8600 0.9900235 #> w_out[11] 0.4033759 1.279229 200.0000 0.9917090 #> w_out[12] 0.4383581 1.780021 200.0000 0.9949652 #> w_out[13] 0.4366914 1.792881 200.0000 0.9928818 #> w_out[14] 0.4086064 1.207952 138.2169 1.0139442 #> w_out[15] 0.3417129 1.625343 200.0000 0.9948078 #> w_out[16] 0.4470843 1.317208 200.0000 1.0030229 #> b_out     0.4753426 1.308099 200.0000 0.9900014 #>  #> Model Fit Information: #> Iterations: 200  #> Warmup: 100  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> Confusion matrix (training with 0.5 cutoff): 7 3  #> Accuracy (training with 0.5 cutoff): 0.7  #> 0.6666667  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values. summary(model_cat) #> Call: #> bnns.default(formula = y_cat ~ -1 + x1 + x2, data = df, L = 3,  #>     nodes = c(32, 16, 8), act_fn = c(3, 2, 2), out_act_fn = 3,  #>     iter = 200, warmup = 100, chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 3  #> Nodes per layer: 32, 16, 8  #> Activation functions: 3, 2, 2  #> Output activation function: 3  #>  #> Posterior Summary (Key Parameters): #>                     mean    se_mean        sd      2.5%        25%          50% #> w_out[1,1]  0.1445477331 0.05847329 0.8269372 -1.456178 -0.5072202  0.075083204 #> w_out[1,2] -0.0512763363 0.07053282 0.9974847 -1.826686 -0.6343180 -0.006513991 #> w_out[1,3]  0.0344279207 0.07382964 1.0441088 -1.900945 -0.7126314  0.054312537 #> w_out[2,1]  0.0577968580 0.09996247 1.3380105 -2.578655 -0.6611543  0.086531062 #> w_out[2,2] -0.0957887406 0.06658559 0.9416625 -1.629212 -0.7497064 -0.030971708 #> w_out[2,3]  0.0029893747 0.07462410 0.9998355 -2.046424 -0.8824320 -0.001885644 #> w_out[3,1]  0.1368857790 0.08409256 1.1892483 -2.049067 -0.6705226 -0.084743369 #> w_out[3,2] -0.1021233280 0.08594312 1.1181669 -2.321632 -0.7664821 -0.107589165 #> w_out[3,3]  0.0260325906 0.06655051 0.9411663 -1.788178 -0.4832624 -0.018023712 #> w_out[4,1] -0.0028844054 0.06693483 0.9466014 -1.712807 -0.7151777 -0.012884313 #> w_out[4,2]  0.0001968483 0.08160187 1.1148632 -2.276743 -0.7590366  0.011934448 #> w_out[4,3] -0.0045651291 0.07408571 1.0477302 -1.832875 -0.6487458 -0.019175158 #> w_out[5,1]  0.0950744976 0.09319237 1.0716226 -1.840047 -0.6311567  0.091425026 #> w_out[5,2] -0.1155975735 0.06978515 0.9650066 -1.741906 -0.8096174 -0.078521709 #> w_out[5,3]  0.0669772476 0.08491057 0.8977346 -1.684854 -0.5377517  0.049692098 #> w_out[6,1]  0.1231706480 0.08119341 0.9705829 -1.624344 -0.5028565  0.083643732 #> w_out[6,2] -0.1589556537 0.08022912 0.8818673 -1.621459 -0.8040660 -0.130048250 #> w_out[6,3]  0.0510863392 0.07949673 0.9623707 -2.086645 -0.4501735  0.041080257 #> w_out[7,1]  0.1730161358 0.08462972 0.9902159 -1.577846 -0.6616558  0.291902049 #> w_out[7,2] -0.0295854306 0.06957140 0.9838882 -1.595405 -0.7977793 -0.115578889 #> w_out[7,3]  0.0379173135 0.06643207 0.9394914 -1.716310 -0.5951511  0.175860162 #> w_out[8,1]  0.1331306062 0.07408174 0.8789793 -1.749949 -0.3459968  0.192039766 #> w_out[8,2] -0.0730116138 0.07666538 0.8912249 -1.742001 -0.6934322 -0.086344702 #> w_out[8,3]  0.0394090795 0.08023090 0.9775812 -1.697123 -0.7157675  0.111447099 #> b_out[1]    0.1792094227 0.05526818 0.7816102 -1.431437 -0.3094301  0.193881645 #> b_out[2]   -0.0519924122 0.06359956 0.8994337 -1.817950 -0.6075309 -0.109846196 #> b_out[3]    0.0934671540 0.06677003 0.8584870 -1.447075 -0.4450014  0.100044898 #>                  75%    97.5%    n_eff      Rhat #> w_out[1,1] 0.8436876 1.542492 200.0000 0.9914225 #> w_out[1,2] 0.5894057 1.812237 200.0000 1.0022855 #> w_out[1,3] 0.8362871 1.804830 200.0000 0.9937128 #> w_out[2,1] 0.9872535 2.318638 179.1617 0.9900582 #> w_out[2,2] 0.4358062 1.507324 200.0000 0.9914146 #> w_out[2,3] 0.7445109 1.799740 179.5142 1.0032525 #> w_out[3,1] 0.9938209 2.597693 200.0000 1.0241183 #> w_out[3,2] 0.8077423 1.785540 169.2743 0.9924486 #> w_out[3,3] 0.5610106 2.028296 200.0000 1.0022836 #> w_out[4,1] 0.6299420 1.713447 200.0000 1.0019255 #> w_out[4,2] 0.9185308 1.952063 186.6564 0.9934494 #> w_out[4,3] 0.5901454 1.975573 200.0000 1.0013152 #> w_out[5,1] 0.6765450 2.259248 132.2279 0.9913469 #> w_out[5,2] 0.5971397 1.741868 191.2205 0.9900270 #> w_out[5,3] 0.5486228 1.950470 111.7821 0.9957489 #> w_out[6,1] 0.7216515 1.927545 142.8972 0.9939585 #> w_out[6,2] 0.4411440 1.509997 120.8210 0.9920938 #> w_out[6,3] 0.6591789 1.596097 146.5501 0.9902139 #> w_out[7,1] 0.7530175 1.959700 136.9033 0.9904907 #> w_out[7,2] 0.6477262 1.790560 200.0000 0.9902553 #> w_out[7,3] 0.7638280 1.718886 200.0000 1.0173475 #> w_out[8,1] 0.7267266 1.617212 140.7781 1.0023445 #> w_out[8,2] 0.5629822 1.524652 135.1375 0.9976894 #> w_out[8,3] 0.7581010 1.992828 148.4644 1.0253875 #> b_out[1]   0.6915429 1.608632 200.0000 0.9926552 #> b_out[2]   0.4992708 1.642560 200.0000 0.9910938 #> b_out[3]   0.6969623 1.617561 165.3120 0.9918786 #>  #> Model Fit Information: #> Iterations: 200  #> Warmup: 100  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> Log-loss (training): 0.9360327  #> AUC (training): 0.9111111  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values."},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"making-predictions","dir":"Articles","previous_headings":"","what":"5. Making Predictions","title":"bnns","text":"predict() function generates predictions new data. format predictions depends output activation function.","code":"# New data test_x <- matrix(runif(10), nrow = 5, ncol = 2) |>   data.frame() |>   `colnames<-`(c(\"x1\", \"x2\"))  # Regression predictions pred_reg <- predict(model_reg, test_x)  # Binary classification predictions pred_bin <- predict(model_bin, test_x)  # Multiclass classification predictions pred_cat <- predict(model_cat, test_x)"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"evaluating-the-model","dir":"Articles","previous_headings":"","what":"6. Evaluating the Model","title":"bnns","text":"bnns package includes utility functions like measure_cont, measure_bin, measure_cat evaluating model performance.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"regression-evaluation","dir":"Articles","previous_headings":"6. Evaluating the Model","what":"Regression Evaluation","title":"bnns","text":"","code":"# True responses test_y <- rnorm(5)  # Evaluate predictions metrics_reg <- measure_cont(obs = test_y, pred = pred_reg) print(metrics_reg) #> $rmse #> [1] 0.9797903 #>  #> $mae #> [1] 0.8840652"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"binary-classification-evaluation","dir":"Articles","previous_headings":"6. Evaluating the Model","what":"Binary Classification Evaluation","title":"bnns","text":"","code":"# True responses test_y_bin <- sample(c(rep(0, 2), rep(1, 3)), 5)  # Evaluate predictions metrics_bin <- measure_bin(obs = test_y_bin, pred = pred_bin) #> Setting levels: control = 0, case = 1 #> Setting direction: controls > cases print(metrics_bin) #> $conf_mat #>    pred_label #> obs 0 #>   0 2 #>   1 3 #>  #> $accuracy #> [1] 0.4 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 2 controls (obs 0) > 3 cases (obs 1). #> Area under the curve: 1 #>  #> $AUC #> [1] 1"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"multiclass-classification-evaluation","dir":"Articles","previous_headings":"6. Evaluating the Model","what":"Multiclass Classification Evaluation","title":"bnns","text":"","code":"# True responses test_y_cat <- factor(sample(letters[1:3], 5, replace = TRUE))  # Evaluate predictions metrics_cat <- measure_cat(obs = test_y_cat, pred = pred_cat) print(metrics_cat) #> $log_loss #> [1] 1.23786 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 3 levels of obs: a, b, c. #> Multi-class area under the curve: 0.1111 #>  #> $AUC #> [1] 0.1111111"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"customized-prior","dir":"Articles","previous_headings":"","what":"7. Customized Prior","title":"bnns","text":"Customized priors can used weights well sigma parameter (regression). show example use Cauchy prior weights multi-classification case.","code":"model_cat_cauchy <- bnns(   y_cat ~ -1 + x1 + x2,   data = df,   L = 3,   nodes = c(32, 16, 8),   act_fn = c(3, 2, 2),   out_act_fn = 3, # Output activation: 3 = Softmax   iter = 2e2,   warmup = 1e2,   chains = 1,   prior_weights = list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) ) # Evaluate predictions metrics_cat_cauchy <- measure_cat(obs = test_y_cat, pred = predict(model_cat_cauchy, test_x)) print(metrics_cat_cauchy) #> $log_loss #> [1] 2.39124 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 3 levels of obs: a, b, c. #> Multi-class area under the curve: 0.2778 #>  #> $AUC #> [1] 0.2777778"},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"notes-on-bayesian-neural-networks","dir":"Articles","previous_headings":"","what":"8. Notes on Bayesian Neural Networks","title":"bnns","text":"Bayesian inference allows use prior knowledge weights. allows uncertainty quantification predictions. Always check convergence diagnostics R-hat values. Use informative priors possible stabilize model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/bnns.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"bnns","text":"details, consult source code GitHub.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Bayesian Neural Networks (BNNs) offer robust framework prediction clinical trials providing posterior distributions predictions. allows probabilistic reasoning, computing probability treatment achieves certain efficacy threshold proportion success. vignette, : 1. Illustrate data preparation clinical trial setting. 2. Fit BNN simulate clinical trial outcomes. 3. Leverage posterior distributions decision-making, calculating posterior probabilities treatment success.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"1. Data Preparation","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Consider hypothetical clinical trial comparing efficacy new treatment placebo. response variable binary, representing treatment success (1) failure (0).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"simulating-data","dir":"Articles","previous_headings":"1. Data Preparation","what":"Simulating Data","title":"Using Bayesian Neural Networks in Clinical Trials","text":"","code":"set.seed(123)  # Simulate predictor variables (e.g., patient covariates) n_subjects <- 100 Age <- runif(n_subjects, 18, 50) # Age in years Dose <- runif(n_subjects, 10, 100) # Dose levels Severity <- runif(n_subjects, 1, 10) # Baseline severity (arbitrary scale)  # Define true probabilities using a nonlinear function beta_0 <- 1 beta_1 <- 0.3 beta_2 <- -0.1 beta_3 <- -0.02 beta_4 <- 0.005  logit_p <- beta_0 + beta_1 * Dose + beta_2 * log(Severity) +   beta_3 * Age^2 + beta_4 * (Age * Dose) p_success <- 1 / (1 + exp(-logit_p)) # Sigmoid transformation  # Simulate binary outcomes Success <- rbinom(n_subjects, size = 1, prob = p_success)  trial_data <- cbind.data.frame(Success, Age, Dose, Severity)  # Split into training and testing train_idx <- sample(seq_len(n_subjects), size = 0.8 * n_subjects) training_data <- trial_data[train_idx, ] test_data <- trial_data[-train_idx, ]"},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"fitting-a-bayesian-neural-network","dir":"Articles","previous_headings":"","what":"2. Fitting a Bayesian Neural Network","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Fit BNN simulated data. use binary classification model logistic sigmoid activation output layer.","code":"# Fit a BNN model <- bnns(   formula = Success ~ -1 + .,   data = training_data,   L = 2, # Number of hidden layers   nodes = c(16, 8), # Nodes per layer   act_fn = c(2, 2), # Activation functions for hidden layers   out_act_fn = 2, # Output activation: logistic sigmoid   iter = 2e2, # Bayesian sampling iterations   warmup = 1e2, # Warmup iterations   chains = 1 # Number of MCMC chains )"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"generating-predictions-with-uncertainty","dir":"Articles","previous_headings":"3. Posterior Predictions","what":"Generating Predictions with Uncertainty","title":"Using Bayesian Neural Networks in Clinical Trials","text":"posterior distribution predictions allows us compute just point estimates also probabilistic metrics. entry posterior_preds represents predicted probability success single posterior sample.","code":"# Generate posterior predictions for the test set posterior_preds <- predict(model, subset(test_data, select = -Success)) head(posterior_preds) # Each row corresponds to a subject, and columns are MCMC samples #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.9613728 0.9329158 0.9556438 0.9426554 0.9534645 0.9044432 0.9168241 #> [2,] 0.2235967 0.1644491 0.2182378 0.3068896 0.1058935 0.4850575 0.5163130 #> [3,] 0.8056729 0.7204448 0.7399269 0.8347028 0.4957216 0.7750339 0.9228643 #> [4,] 0.9503497 0.9307014 0.9336504 0.8962607 0.9258469 0.8921747 0.9150469 #> [5,] 0.3995912 0.2486405 0.3396684 0.2797588 0.1349837 0.3642235 0.4686999 #> [6,] 0.5136836 0.3549363 0.4026785 0.4110749 0.1543610 0.4230202 0.5971009 #>           [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14] #> [1,] 0.9111997 0.9121889 0.8625658 0.9567358 0.9542246 0.9602404 0.9357228 #> [2,] 0.3942688 0.4497209 0.3828122 0.6100914 0.3841118 0.6776734 0.8269675 #> [3,] 0.9189465 0.8568211 0.8587497 0.9072963 0.9427342 0.8775716 0.8878071 #> [4,] 0.8988923 0.8243679 0.6926867 0.9477703 0.8762663 0.9620489 0.8437045 #> [5,] 0.6172400 0.4568082 0.6192524 0.1567513 0.3564466 0.5361770 0.4396265 #> [6,] 0.7239343 0.6282899 0.7178172 0.3237377 0.3995538 0.6545576 0.5689066 #>          [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21] #> [1,] 0.9062137 0.8199521 0.9797988 0.9247451 0.8758179 0.9106642 0.9279392 #> [2,] 0.4204703 0.3761443 0.3498603 0.5307114 0.3729644 0.4016827 0.4966943 #> [3,] 0.7477193 0.9216222 0.9516617 0.7800093 0.7689832 0.7890541 0.8993579 #> [4,] 0.8732688 0.7904393 0.9490875 0.9284750 0.8721311 0.8871057 0.8851561 #> [5,] 0.2170031 0.4329073 0.5254285 0.2187272 0.2079545 0.2144293 0.3039564 #> [6,] 0.3199969 0.5176712 0.7143767 0.4021149 0.3386800 0.3075837 0.4277087 #>          [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28] #> [1,] 0.9381318 0.9401079 0.9177812 0.9502537 0.8893577 0.9132688 0.9306638 #> [2,] 0.7239841 0.2476447 0.3268315 0.5038306 0.4654164 0.4915736 0.5267577 #> [3,] 0.8866630 0.6951838 0.8274708 0.7791003 0.6198703 0.7538855 0.8055793 #> [4,] 0.9247410 0.9396331 0.9158460 0.9269720 0.8477860 0.9098030 0.7691952 #> [5,] 0.4708130 0.3320307 0.5031485 0.2967139 0.1787833 0.3972591 0.2819297 #> [6,] 0.6304738 0.3819711 0.6011345 0.3874847 0.2662456 0.4966650 0.3946501 #>          [,29]     [,30]     [,31]     [,32]     [,33]     [,34]     [,35] #> [1,] 0.9764351 0.8111083 0.9331689 0.9278026 0.9802747 0.8755287 0.8758250 #> [2,] 0.5271400 0.3400261 0.3769680 0.3956324 0.6121979 0.5617213 0.2089155 #> [3,] 0.9225364 0.9392014 0.7829915 0.6920177 0.8974057 0.7561331 0.6041600 #> [4,] 0.9355604 0.7051606 0.9230535 0.9060591 0.9773210 0.8250347 0.8357104 #> [5,] 0.4465772 0.3130289 0.3673487 0.2285491 0.3752401 0.4501153 0.1125821 #> [6,] 0.4947116 0.4501688 0.4598175 0.2546649 0.4098976 0.4671363 0.1844588 #>          [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42] #> [1,] 0.9404006 0.9840714 0.8828102 0.9428312 0.8524162 0.8184400 0.9453940 #> [2,] 0.4458837 0.3849919 0.5301549 0.6083645 0.6205557 0.2320280 0.2448369 #> [3,] 0.7345420 0.9789656 0.7607266 0.8524568 0.6886955 0.6944182 0.8688881 #> [4,] 0.9378620 0.9611236 0.8659909 0.9409270 0.8480691 0.8211074 0.9204905 #> [5,] 0.2539474 0.5429007 0.4773449 0.3625701 0.3141186 0.2736207 0.2033742 #> [6,] 0.3475937 0.6271542 0.5603578 0.4458035 0.4462321 0.3121366 0.3033622 #>          [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49] #> [1,] 0.9314521 0.8926037 0.9187898 0.9669054 0.9414073 0.8982879 0.8853734 #> [2,] 0.3891865 0.3001002 0.2933510 0.3695236 0.1537747 0.4539966 0.4499136 #> [3,] 0.8428539 0.4215342 0.7616756 0.9482543 0.8841856 0.7722048 0.8569334 #> [4,] 0.9288824 0.8853153 0.9181569 0.9273490 0.8586674 0.9032350 0.8870742 #> [5,] 0.4357125 0.1985050 0.2050060 0.3660159 0.1875264 0.3133900 0.2402087 #> [6,] 0.5310861 0.2510564 0.3051820 0.5395314 0.2703691 0.4108854 0.3354512 #>          [,50]     [,51]     [,52]     [,53]     [,54]     [,55]     [,56] #> [1,] 0.8850330 0.9481938 0.7631612 0.9919739 0.9683620 0.9657593 0.9215794 #> [2,] 0.3060510 0.5348796 0.2743101 0.4085100 0.2210106 0.3131029 0.6377823 #> [3,] 0.5835751 0.9198466 0.8657223 0.9274107 0.9510812 0.9178727 0.8503718 #> [4,] 0.8812970 0.9548685 0.7177186 0.9895322 0.9137324 0.9551138 0.9159162 #> [5,] 0.3237551 0.4597274 0.5527613 0.3725069 0.6053935 0.4144107 0.2121038 #> [6,] 0.3858880 0.5687552 0.6816026 0.5418036 0.8022279 0.4778728 0.3292657 #>          [,57]     [,58]     [,59]     [,60]     [,61]     [,62]     [,63] #> [1,] 0.9215079 0.9661708 0.9287160 0.9045619 0.9479006 0.9089535 0.9351302 #> [2,] 0.2961248 0.2111699 0.4803532 0.6683922 0.4720353 0.5020279 0.2396743 #> [3,] 0.4998061 0.8264248 0.8811049 0.8106474 0.7494190 0.8045081 0.8647755 #> [4,] 0.9100478 0.9461816 0.8951531 0.8896397 0.9472594 0.8929029 0.9101152 #> [5,] 0.2378558 0.3725571 0.3979848 0.3867989 0.4796191 0.2040099 0.5845103 #> [6,] 0.2780328 0.4603849 0.5585999 0.5533180 0.5638147 0.2781013 0.6481464 #>          [,64]     [,65]     [,66]     [,67]     [,68]     [,69]     [,70] #> [1,] 0.9362955 0.9810709 0.9391178 0.9046810 0.9549073 0.9062767 0.8484791 #> [2,] 0.4080229 0.3473553 0.4083197 0.4326079 0.1467606 0.5259679 0.4228981 #> [3,] 0.9423801 0.8537209 0.8669052 0.9233210 0.7958136 0.7725000 0.8445934 #> [4,] 0.9287343 0.9622713 0.9031129 0.8694526 0.9473404 0.8945832 0.8036760 #> [5,] 0.3892443 0.5002476 0.4874131 0.4154533 0.1314282 0.3296129 0.3185250 #> [6,] 0.5990180 0.6087540 0.5214094 0.6181487 0.2507183 0.3677774 0.3964019 #>          [,71]     [,72]     [,73]     [,74]     [,75]     [,76]     [,77] #> [1,] 0.9600188 0.8784986 0.8031698 0.8698130 0.9590640 0.9356778 0.8546193 #> [2,] 0.4771104 0.3633745 0.4852906 0.4100818 0.5147932 0.4720721 0.2328137 #> [3,] 0.8487589 0.4684934 0.7916721 0.7484900 0.9514166 0.5351346 0.5133759 #> [4,] 0.9522802 0.8696047 0.8136503 0.7677945 0.9285820 0.9272583 0.8542228 #> [5,] 0.3935640 0.2954323 0.3111445 0.2410921 0.4219474 0.2339606 0.1817854 #> [6,] 0.5207915 0.3229921 0.3531202 0.3067603 0.5980656 0.3287756 0.3101685 #>          [,78]     [,79]     [,80]     [,81]     [,82]     [,83]     [,84] #> [1,] 0.8335529 0.8521607 0.9394700 0.8722404 0.8522924 0.9677636 0.8942321 #> [2,] 0.4115653 0.4729379 0.6018438 0.2640329 0.2789536 0.3542480 0.5991980 #> [3,] 0.7733507 0.6755266 0.9085519 0.7678251 0.8120000 0.7112474 0.9342559 #> [4,] 0.8183802 0.8506890 0.8483583 0.8773471 0.8475879 0.9621285 0.8525200 #> [5,] 0.3257125 0.3322537 0.4073050 0.2507164 0.3403176 0.4340413 0.4826738 #> [6,] 0.4518530 0.4288814 0.5365180 0.3403906 0.4684776 0.5470473 0.5459098 #>          [,85]     [,86]     [,87]     [,88]     [,89]     [,90]     [,91] #> [1,] 0.8995534 0.8023447 0.8201273 0.9409108 0.8657193 0.9402508 0.9775874 #> [2,] 0.3226545 0.4213005 0.2434076 0.7454907 0.6010352 0.4741427 0.6699810 #> [3,] 0.5719712 0.7613711 0.7760065 0.9077010 0.9179295 0.7328091 0.9779914 #> [4,] 0.8285682 0.7752236 0.7646138 0.9383748 0.7719000 0.9277486 0.9733046 #> [5,] 0.1841121 0.4251285 0.4915524 0.2123416 0.5625906 0.2996497 0.4419641 #> [6,] 0.2261076 0.5789391 0.5530188 0.3170862 0.6148680 0.4777281 0.7042311 #>          [,92]     [,93]     [,94]     [,95]     [,96]     [,97]     [,98] #> [1,] 0.9600465 0.8925409 0.9593908 0.8879047 0.9538929 0.8962047 0.9588745 #> [2,] 0.5688328 0.1968743 0.3527832 0.4832171 0.7573290 0.5397747 0.4212425 #> [3,] 0.7624066 0.6606584 0.5792619 0.8037181 0.9501151 0.7816517 0.8668333 #> [4,] 0.9538866 0.8353541 0.9242324 0.8749433 0.9435723 0.9024435 0.9450509 #> [5,] 0.3328679 0.2459936 0.1511894 0.6797451 0.3450042 0.3184492 0.2439964 #> [6,] 0.3710686 0.3690858 0.3234398 0.7279696 0.5505594 0.5475009 0.4242612 #>          [,99]    [,100] #> [1,] 0.9671279 0.9526307 #> [2,] 0.1738080 0.2218101 #> [3,] 0.8845034 0.7718468 #> [4,] 0.9691098 0.7774003 #> [5,] 0.5791478 0.3798798 #> [6,] 0.6863167 0.5275403"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"threshold-based-decision-making","dir":"Articles","previous_headings":"4. Posterior Probability of Treatment Success","what":"Threshold-Based Decision-Making","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Suppose define treatment success predicted probability ≥ 0.6. can compute posterior probability threshold met subject.","code":"# Compute posterior probabilities of success (p_hat ≥ 0.6) success_threshold <- 0.6 posterior_probs_success <- rowMeans(posterior_preds >= success_threshold) head(posterior_probs_success) #> [1] 1.00 0.14 0.91 1.00 0.04 0.17"},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"proportion-of-subjects-likely-to-achieve-success","dir":"Articles","previous_headings":"4. Posterior Probability of Treatment Success","what":"Proportion of Subjects Likely to Achieve Success","title":"Using Bayesian Neural Networks in Clinical Trials","text":"Next, calculate posterior probability certain proportion subjects treatment group achieve success.","code":"# Define success proportion threshold prop_success_threshold <- 0.7  # Simulate posterior proportion of success posterior_success_proportion <- colMeans(posterior_preds >= success_threshold)  # Posterior probability that ≥ 70% of subjects achieve success posterior_prob_high_success <- mean(posterior_success_proportion >= prop_success_threshold) posterior_prob_high_success #> [1] 0.16"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"posterior-predictive-distribution","dir":"Articles","previous_headings":"5. Visualizing Posterior Insights","what":"Posterior Predictive Distribution","title":"Using Bayesian Neural Networks in Clinical Trials","text":"","code":"library(ggplot2)  # Plot posterior probabilities of success for individual subjects ggplot(data.frame(Subject = seq_len(nrow(test_data)), Prob = posterior_probs_success), aes(x = Subject, y = Prob)) +   geom_bar(stat = \"identity\", fill = \"blue\") +   geom_hline(yintercept = success_threshold, color = \"red\", linetype = \"dashed\") +   labs(     title = \"Posterior Probability of Treatment Success\",     x = \"Subject\",     y = \"Posterior Probability\"   )"},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"distribution-of-success-proportion","dir":"Articles","previous_headings":"5. Visualizing Posterior Insights","what":"Distribution of Success Proportion","title":"Using Bayesian Neural Networks in Clinical Trials","text":"","code":"# Histogram of posterior success proportions ggplot(data.frame(SuccessProp = posterior_success_proportion), aes(x = SuccessProp)) +   geom_histogram(fill = \"green\", bins = 20) +   geom_vline(xintercept = prop_success_threshold, color = \"red\", linetype = \"dashed\") +   labs(     title = \"Posterior Distribution of Success Proportion\",     x = \"Proportion of Subjects Achieving Success\",     y = \"Frequency\"   )"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"bayesian-probability-threshold","dir":"Articles","previous_headings":"6. Clinical Trial Decision-Making","what":"Bayesian Probability Threshold","title":"Using Bayesian Neural Networks in Clinical Trials","text":"posterior probability can guide decision-making. example: - posterior_prob_high_success > 0.9, consider treatment effective. - posterior_prob_high_success < 0.1, consider treatment ineffective. - Otherwise, collect data refine model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/articles/ct_app.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"7. Conclusion","title":"Using Bayesian Neural Networks in Clinical Trials","text":"bnns package empowers clinical trial analysts leverage Bayesian Neural Networks predictive modeling decision-making. utilizing posterior distributions, can: - Quantify uncertainty predictions. - Make informed decisions treatment efficacy. - Evaluate trial outcomes based predefined success criteria. probabilistic framework particularly valuable scenarios uncertainty plays critical role decision-making, early-phase clinical trials. ```","code":""},{"path":"https://swarnendu-stat.github.io/bnns/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Swarnendu Chatterjee. Author, maintainer, copyright holder.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chatterjee S (2025). bnns: Bayesian Neural Network Stan. R package version 0.0.0.9000, https://swarnendu-stat.github.io/bnns/, https://github.com/swarnendu-stat/bnns.","code":"@Manual{,   title = {bnns: Bayesian Neural Network with Stan},   author = {Swarnendu Chatterjee},   year = {2025},   note = {R package version 0.0.0.9000, https://swarnendu-stat.github.io/bnns/},   url = {https://github.com/swarnendu-stat/bnns}, }"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"bnns-","dir":"","previous_headings":"","what":"Bayesian Neural Network with Stan","title":"Bayesian Neural Network with Stan","text":"bnns package provides tools fit Bayesian Neural Networks (BNNs) regression classification problems. designed flexible, supporting various network architectures, activation functions, output types, making suitable simple complex data analysis tasks.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Bayesian Neural Network with Stan","text":"Support multi-layer neural networks customizable architecture. Choice activation functions (e.g., sigmoid, ReLU, tanh). Outputs regression (continuous response) classification (binary multiclass). Choice prior distributions weights, biases sigma (regression). Bayesian inference, providing posterior distributions predictions parameters. Applications domains clinical trials, predictive modeling, .","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"installation-stable-cran-version","dir":"","previous_headings":"","what":"Installation (stable CRAN version)","title":"Bayesian Neural Network with Stan","text":"install bnns package CRAN, use following:","code":"install.packages(\"bnns\")"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"installation-development-version","dir":"","previous_headings":"","what":"Installation (development version)","title":"Bayesian Neural Network with Stan","text":"install bnns package GitHub, use following:","code":"# Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") }  # Install bnns devtools::install_github(\"swarnendu-stat/bnns\")"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_1-simulate-data","dir":"","previous_headings":"Getting Started","what":"1. Simulate Data","title":"Bayesian Neural Network with Stan","text":"example simulate data regression:","code":"set.seed(123) df <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10))"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_2-fit-a-bnn-model","dir":"","previous_headings":"Getting Started","what":"2. Fit a BNN Model","title":"Bayesian Neural Network with Stan","text":"fit Bayesian Neural Network:","code":"library(bnns)  model <- bnns(y ~ -1 + x1 + x2,   data = df, L = 2, nodes = c(10, 8), act_fn = c(2, 3), out_act_fn = 1,   iter = 1e2, warmup = 5e1, chains = 1, seed = 123 )"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_3-model-summary","dir":"","previous_headings":"Getting Started","what":"3. Model Summary","title":"Bayesian Neural Network with Stan","text":"Summarize fitted model:","code":"summary(model) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = df, L = 2, nodes = c(10,  #>     8), act_fn = c(2, 3), out_act_fn = 1, iter = 100, warmup = 50,  #>     chains = 1, seed = 123) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 2  #> Nodes per layer: 10, 8  #> Activation functions: 2, 3  #> Output activation function: 1  #>  #> Posterior Summary (Key Parameters): #>                  mean    se_mean        sd       2.5%        25%         50% #> w_out[1]  0.046800977 0.10811114 0.8557186 -1.4270932 -0.4344960 -0.06862144 #> w_out[2]  0.115880441 0.13319242 0.9380149 -1.5099093 -0.4571497  0.07552893 #> w_out[3] -0.066245807 0.13469714 0.9291137 -1.6212377 -0.8311266 -0.12900962 #> w_out[4] -0.103740675 0.09216505 0.7583326 -1.2582963 -0.6014192 -0.19034730 #> w_out[5] -0.136488041 0.18784111 0.9671970 -1.8916180 -0.9706804  0.03012556 #> w_out[6]  0.017294924 0.12168441 0.8081913 -1.3438848 -0.6034305 -0.08493691 #> w_out[7]  0.135528160 0.12928282 0.9187944 -1.6799768 -0.4365047  0.12827137 #> w_out[8] -0.005637373 0.13525773 0.8900234 -1.1675289 -0.6722286 -0.16385458 #> b_out     0.046645154 0.12189640 0.9242511 -1.5332072 -0.4610939  0.10835165 #> sigma     0.878312806 0.03548238 0.2349594  0.5149699  0.7192499  0.86315178 #>                75%    97.5%    n_eff      Rhat #> w_out[1] 0.4759225 1.902804 62.64999 0.9982886 #> w_out[2] 0.7984308 1.885203 49.59758 0.9804256 #> w_out[3] 0.6134859 1.725527 47.57962 0.9826598 #> w_out[4] 0.3863629 1.198198 67.69972 0.9997155 #> w_out[5] 0.5359341 1.490437 26.51237 0.9805991 #> w_out[6] 0.6106351 1.368162 44.11218 0.9848957 #> w_out[7] 0.6400878 1.549369 50.50740 0.9820679 #> w_out[8] 0.6303301 1.985030 43.29908 1.0042768 #> b_out    0.6232685 1.547132 57.49078 0.9800999 #> sigma    1.0105743 1.324582 43.84903 0.9809578 #>  #> Model Fit Information: #> Iterations: 100  #> Warmup: 50  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> RMSE (training): 0.6676959  #> MAE (training): 0.5158743  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values."},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_4-predictions","dir":"","previous_headings":"Getting Started","what":"4. Predictions","title":"Bayesian Neural Network with Stan","text":"Make predictions using trained model:","code":"pred <- predict(model)"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"id_5-visualization","dir":"","previous_headings":"Getting Started","what":"5. Visualization","title":"Bayesian Neural Network with Stan","text":"Visualize true vs predicted values regression:","code":"plot(df$y, rowMeans(pred), main = \"True vs Predicted\", xlab = \"True Values\", ylab = \"Predicted Values\") abline(0, 1, col = \"red\")"},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"regression-example-with-custom-priors","dir":"","previous_headings":"Applications","what":"Regression Example (with custom priors)","title":"Bayesian Neural Network with Stan","text":"Use bnns regression analysis model continuous outcomes, predicting patient biomarkers clinical trials.","code":"model <- bnns(y ~ -1 + x1 + x2,   data = df, L = 2, nodes = c(10, 8), act_fn = c(2, 3), out_act_fn = 1,   iter = 1e2, warmup = 5e1, chains = 1, seed = 123,   prior_weights = list(dist = \"uniform\", params = list(alpha = -1, beta = 1)),   prior_bias =  list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)),   prior_sigma = list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) )"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"classification-example","dir":"","previous_headings":"Applications","what":"Classification Example","title":"Bayesian Neural Network with Stan","text":"binary multiclass classification, set out_act_fn 2 (binary) 3 (multiclass). example:","code":"# Simulate binary classification data df <- data.frame(x1 = runif(10), x2 = runif(10), y = sample(0:1, 10, replace = TRUE))  # Fit a binary classification BNN model <- bnns(y ~ -1 + x1 + x2, data = df, L = 2, nodes = c(16, 8), act_fn = c(3, 2), out_act_fn = 2, iter = 1e2, warmup = 5e1, chains = 1)"},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"clinical-trial-applications","dir":"","previous_headings":"Applications","what":"Clinical Trial Applications","title":"Bayesian Neural Network with Stan","text":"Explore posterior probabilities estimate treatment effects success probabilities clinical trials. example, calculate posterior probability achieving clinically meaningful outcome given population.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Bayesian Neural Network with Stan","text":"Detailed vignettes available guide various applications package. See help(bnns) information bnns function arguments.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Bayesian Neural Network with Stan","text":"Contributions welcome! Please raise issues submit pull requests GitHub.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Bayesian Neural Network with Stan","text":"package licensed Apache License. See LICENSE details.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"Fits Bayesian Neural Network (BNN) model using formula interface. function parses formula data create input feature matrix target vector, fits model using bnns.default.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"","code":"# Default S3 method bnns(   formula,   data = list(),   L = 1,   nodes = 16,   act_fn = 2,   out_act_fn = 1,   iter = 1000,   warmup = 200,   thin = 1,   chains = 2,   cores = 2,   seed = 123,   prior_weights = NULL,   prior_bias = NULL,   prior_sigma = NULL,   verbose = FALSE,   refresh = max(iter/10, 1),   normalize = TRUE,   ... )"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"formula symbolic description model fitted. formula specify response variable predictors (e.g., y ~ x1 + x2). data data frame list containing variables model. Default empty list. L integer specifying number hidden layers neural network. Default 1. nodes integer vector specifying number nodes hidden layer. single value provided, applied layers. Default 16. act_fn integer vector specifying activation function(s) hidden layers. Options : 1 tanh 2 sigmoid (default) 3 ReLU 4 softplus out_act_fn integer specifying activation function output layer. Options : 1 linear (default) 2 sigmoid 3 softmax iter integer specifying total number iterations Stan sampler. Default 1e3. warmup integer specifying number warmup iterations Stan sampler. Default 2e2. thin integer specifying thinning interval Stan samples. Default 1. chains integer specifying number Markov chains. Default 2. cores integer specifying number CPU cores use parallel sampling. Default 2. seed integer specifying random seed reproducibility. Default 123. prior_weights list specifying prior distribution weights neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_weights NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_bias list specifying prior distribution biases neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_bias NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_sigma list specifying prior distribution sigma parameter regression models (out_act_fn = 1). allows setting priors standard deviation residuals. list must include two components: dist: character string specifying distribution type. Supported values \"half-normal\" \"inverse-gamma\". params: named list specifying parameters chosen distribution: \"half-normal\": Provide sd (standard deviation half-normal distribution). \"inverse-gamma\": Provide shape (shape parameter) scale (scale parameter). prior_sigma NULL, default prior half-normal(0, 1) distribution. example: list(dist = \"half_normal\", params = list(mean = 0, sd = 1)) list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) verbose TRUE FALSE: flag indicating whether print intermediate output Stan console, might helpful model debugging. refresh refresh (integer) can used control often progress sampling reported (.e. show progress every refresh iterations). default, refresh = max(iter/10, 1). progress indicator turned refresh <= 0. normalize Logical. TRUE (default), input predictors normalized zero mean unit variance training. Normalization ensures stable efficient Bayesian sampling standardizing input scale, particularly beneficial neural network training. FALSE, normalization applied, assumed input data already pre-processed appropriately. ... Currently use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"object class \"bnns\" containing fitted model associated information, including: fit: fitted Stan model object. data: list containing processed training data. call: matched function call. formula: formula used model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"function uses provided formula data generate design matrix predictors response vector. calls helper function bnns_train fit Bayesian Neural Network model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.default.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Neural Network Model Using Formula(default) Interface — bnns.default","text":"","code":"# Example usage with formula interface: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 3,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.005 seconds (Warm-up) #> Chain 1:                0.006 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Function for Fitting Bayesian Neural Network Models — bnns","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"generic function fitting Bayesian Neural Network (BNN) models. dispatches methods based class input data.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"","code":"bnns(   formula,   data = list(),   L = 1,   nodes = 16,   act_fn = 2,   out_act_fn = 1,   iter = 1000,   warmup = 200,   thin = 1,   chains = 2,   cores = 2,   seed = 123,   prior_weights = NULL,   prior_bias = NULL,   prior_sigma = NULL,   verbose = FALSE,   refresh = max(iter/10, 1),   normalize = TRUE,   ... )"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"formula symbolic description model fitted. formula specify response variable predictors (e.g., y ~ x1 + x2). y must continuous regression (out_act_fn = 1), numeric 0/1 binary classification (out_act_fn = 2), factor least 3 levels multi-classification (out_act_fn = 3). data data frame list containing variables model. Default empty list. L integer specifying number hidden layers neural network. Default 1. nodes integer vector specifying number nodes hidden layer. single value provided, applied layers. Default 16. act_fn integer vector specifying activation function(s) hidden layers. Options : 1 tanh 2 sigmoid (default) 3 ReLU 4 softplus out_act_fn integer specifying activation function output layer. Options : 1 linear (default) 2 sigmoid 3 softmax iter integer specifying total number iterations Stan sampler. Default 1e3. warmup integer specifying number warmup iterations Stan sampler. Default 2e2. thin integer specifying thinning interval Stan samples. Default 1. chains integer specifying number Markov chains. Default 2. cores integer specifying number CPU cores use parallel sampling. Default 2. seed integer specifying random seed reproducibility. Default 123. prior_weights list specifying prior distribution weights neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_weights NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_bias list specifying prior distribution biases neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_bias NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_sigma list specifying prior distribution sigma parameter regression models (out_act_fn = 1). allows setting priors standard deviation residuals. list must include two components: dist: character string specifying distribution type. Supported values \"half-normal\" \"inverse-gamma\". params: named list specifying parameters chosen distribution: \"half-normal\": Provide sd (standard deviation half-normal distribution). \"inverse-gamma\": Provide shape (shape parameter) scale (scale parameter). prior_sigma NULL, default prior half-normal(0, 1) distribution. example: list(dist = \"half_normal\", params = list(mean = 0, sd = 1)) list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) verbose TRUE FALSE: flag indicating whether print intermediate output Stan console, might helpful model debugging. refresh refresh (integer) can used control often progress sampling reported (.e. show progress every refresh iterations). default, refresh = max(iter/10, 1). progress indicator turned refresh <= 0. normalize Logical. TRUE (default), input predictors normalized zero mean unit variance training. Normalization ensures stable efficient Bayesian sampling standardizing input scale, particularly beneficial neural network training. FALSE, normalization applied, assumed input data already pre-processed appropriately. ... Currently use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"result method dispatched class input data. Typically, object class \"bnns\" containing fitted model associated information.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"function serves generic interface different methods fitting Bayesian Neural Networks. specific method dispatched depends class input arguments, allowing flexibility types inputs supported.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"Bishop, C.M., 1995. Neural networks pattern recognition. Oxford university press. Carpenter, B., Gelman, ., Hoffman, M.D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M.., Guo, J., Li, P. Riddell, ., 2017. Stan: probabilistic programming language. Journal statistical software, 76. Neal, R.M., 2012. Bayesian learning neural networks (Vol. 118). Springer Science & Business Media.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generic Function for Fitting Bayesian Neural Network Models — bnns","text":"","code":"# Example usage with formula interface: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 1,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.8e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.006 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:   # See the documentation for bnns.default for more details on the default implementation."},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function for training the BNN — bnns_train","title":"Internal function for training the BNN — bnns_train","text":"function performs actual fitting Bayesian Neural Network. called exported bnns methods intended direct use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function for training the BNN — bnns_train","text":"","code":"bnns_train(   train_x,   train_y,   L = 1,   nodes = 16,   act_fn = 2,   out_act_fn = 1,   iter = 1000,   warmup = 200,   thin = 1,   chains = 2,   cores = 2,   seed = 123,   prior_weights = NULL,   prior_bias = NULL,   prior_sigma = NULL,   verbose = FALSE,   refresh = max(iter/10, 1),   normalize = TRUE,   ... )"},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function for training the BNN — bnns_train","text":"train_x numeric matrix representing input features (predictors) training. Rows correspond observations, columns correspond features. train_y numeric vector representing target values training. length must match number rows train_x. L integer specifying number hidden layers neural network. Default 1. nodes integer vector specifying number nodes hidden layer. single value provided, applied layers. Default 16. act_fn integer vector specifying activation function(s) hidden layers. Options : 1 tanh 2 sigmoid (default) 3 ReLU 4 softplus out_act_fn integer specifying activation function output layer. Options : 1 linear (default) 2 sigmoid 3 softmax iter integer specifying total number iterations Stan sampler. Default 1e3. warmup integer specifying number warmup iterations Stan sampler. Default 2e2. thin integer specifying thinning interval Stan samples. Default 1. chains integer specifying number Markov chains. Default 2. cores integer specifying number CPU cores use parallel sampling. Default 2. seed integer specifying random seed reproducibility. Default 123. prior_weights list specifying prior distribution weights neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_weights NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_bias list specifying prior distribution biases neural network. list must include two components: dist: character string specifying distribution type. Supported values \"normal\", \"uniform\", \"cauchy\". params: named list specifying parameters chosen distribution: \"normal\": Provide mean (mean distribution) sd (standard deviation). \"uniform\": Provide alpha (lower bound) beta (upper bound). \"cauchy\": Provide mu (location parameter) sigma (scale parameter). prior_bias NULL, default prior normal(0, 1) distribution. example: list(dist = \"normal\", params = list(mean = 0, sd = 1)) list(dist = \"uniform\", params = list(alpha = -1, beta = 1)) list(dist = \"cauchy\", params = list(mu = 0, sigma = 2.5)) prior_sigma list specifying prior distribution sigma parameter regression models (out_act_fn = 1). allows setting priors standard deviation residuals. list must include two components: dist: character string specifying distribution type. Supported values \"half-normal\" \"inverse-gamma\". params: named list specifying parameters chosen distribution: \"half-normal\": Provide sd (standard deviation half-normal distribution). \"inverse-gamma\": Provide shape (shape parameter) scale (scale parameter). prior_sigma NULL, default prior half-normal(0, 1) distribution. example: list(dist = \"half_normal\", params = list(mean = 0, sd = 1)) list(dist = \"inv_gamma\", params = list(alpha = 1, beta = 1)) verbose TRUE FALSE: flag indicating whether print intermediate output Stan console, might helpful model debugging. refresh refresh (integer) can used control often progress sampling reported (.e. show progress every refresh iterations). default, refresh = max(iter/10, 1). progress indicator turned refresh <= 0. normalize Logical. TRUE (default), input predictors normalized zero mean unit variance training. Normalization ensures stable efficient Bayesian sampling standardizing input scale, particularly beneficial neural network training. FALSE, normalization applied, assumed input data already pre-processed appropriately. ... Currently use.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function for training the BNN — bnns_train","text":"object class \"bnns\" containing following components: fit fitted Stan model object. call matched call. data list containing Stan data used model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function for training the BNN — bnns_train","text":"function uses generate_stan_code function dynamically generate Stan code based specified number layers nodes. Stan used fit Bayesian Neural Network.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/bnns_train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal function for training the BNN — bnns_train","text":"","code":"# Example usage: train_x <- matrix(runif(20), nrow = 10, ncol = 2) train_y <- rnorm(10) model <- bnns:::bnns_train(train_x, train_y,   L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.007 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.012 seconds (Total) #> Chain 1:   # Access Stan model fit model$fit #> Inference for Stan model: anon_model. #> 1 chains, each with iter=100; warmup=50; thin=1;  #> post-warmup draws per chain=50, total post-warmup draws=50. #>  #>           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat #> w1[1,1]   0.06    0.16 0.97 -1.70 -0.50  0.00  0.82  1.76    38 1.05 #> w1[1,2]   0.03    0.19 0.98 -1.40 -0.69 -0.11  0.41  2.47    28 1.02 #> w1[2,1]   0.10    0.13 0.86 -1.40 -0.47  0.12  0.79  1.38    47 1.00 #> w1[2,2]  -0.08    0.21 1.15 -2.24 -0.71 -0.01  0.84  1.68    30 0.98 #> b1[1]     0.08    0.14 0.89 -1.38 -0.64  0.21  0.70  1.80    43 1.07 #> b1[2]    -0.21    0.13 1.01 -2.06 -0.86 -0.37  0.66  1.69    63 0.98 #> w_out[1] -0.15    0.16 0.90 -1.52 -0.69 -0.25  0.46  1.67    30 1.10 #> w_out[2]  0.10    0.20 1.02 -1.63 -0.57  0.20  0.72  2.01    25 0.98 #> b_out     0.14    0.09 0.63 -0.94 -0.29  0.12  0.52  1.39    49 1.01 #> sigma     0.70    0.03 0.20  0.45  0.57  0.65  0.78  1.21    51 1.00 #> lp__     -5.84    0.34 1.95 -9.40 -7.00 -5.54 -4.25 -2.99    34 0.98 #>  #> Samples were drawn using NUTS(diag_e) at Fri Jan  3 04:37:29 2025. #> For each parameter, n_eff is a crude measure of effective sample size, #> and Rhat is the potential scale reduction factor on split chains (at  #> convergence, Rhat=1)."},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Stan Code Based on Output Activation Function — generate_stan_code","title":"Generate Stan Code Based on Output Activation Function — generate_stan_code","text":"function serves wrapper generate Stan code Bayesian neural networks tailored different types response variables. Based specified output activation function (out_act_fn), delegates code generation appropriate function continuous, binary, categorical response models.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Stan Code Based on Output Activation Function — generate_stan_code","text":"","code":"generate_stan_code(num_layers, nodes, out_act_fn = 1)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Stan Code Based on Output Activation Function — generate_stan_code","text":"num_layers integer specifying number hidden layers neural network. nodes vector integers, element specifies number nodes corresponding hidden layer. length vector must match num_layers. out_act_fn integer specifying output activation function, determining type response variable. Supported values : 1: Continuous response (identity function output layer). 2: Binary response (sigmoid function output layer). 3: Categorical response (softmax function output layer).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Stan Code Based on Output Activation Function — generate_stan_code","text":"character string containing Stan code specified Bayesian neural network model. Stan model includes data, parameters, transformed parameters, model blocks, adjusted based specified response type.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Stan Code Based on Output Activation Function — generate_stan_code","text":"function dynamically calls one following functions based value out_act_fn: Continuous response: Calls generate_stan_code_cont. Binary response: Calls generate_stan_code_bin. Categorical response: Calls generate_stan_code_cat. unsupported value provided out_act_fn, function throws error. generated Stan code adapted response type, including appropriate likelihood functions transformations.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Stan Code Based on Output Activation Function — generate_stan_code","text":"","code":"# Generate Stan code for a continuous response model stan_code <- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 1) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   vector[n] y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #>   real<lower=0> sigma; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   sigma ~ PRIOR_SIGMA; #>   y ~ normal(y_hat, sigma); #> }  # Generate Stan code for a binary response model stan_code <- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 2) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=0, upper=1> y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   y ~ bernoulli_logit(y_hat); #> }  # Generate Stan code for a categorical response model stan_code <- generate_stan_code(num_layers = 2, nodes = c(10, 5), out_act_fn = 3) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=1> y; #>   int<lower=1> act_fn[L]; #>   int<lower=2> K; // Number of categories #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   matrix[nodes[L], K] w_out; #>   vector[K] b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   matrix[n, K] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   y_hat = a2 * w_out + rep_matrix(b_out', n); #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   to_vector(w_out) ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   for (i in 1:n) y[i] ~ categorical_logit(y_hat[i]'); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","title":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"function generates Stan code Bayesian neural network model designed predict binary response variables. Stan code dynamically constructed based specified number hidden layers nodes per layer. supports various activation functions hidden layers, including tanh, sigmoid, softplus relu. model uses Bernoulli likelihood binary outcomes.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"","code":"generate_stan_code_bin(num_layers, nodes)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"num_layers integer specifying number hidden layers neural network. nodes vector integers, element specifies number nodes corresponding hidden layer. length vector must match num_layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"character string containing Stan code specified Bayesian neural network model. Stan model includes data, parameters, transformed parameters, model blocks. code adjusted based whether network one multiple hidden layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"generated Stan code models binary response variable using neural network. hidden layers apply specified activation functions, output layer applies logistic function predict probability binary outcome. one hidden layer: function simplifies Stan code structure. multiple hidden layers: code dynamically includes additional layers based input arguments. Supported activation functions hidden layers: 1: Tanh 2: Sigmoid 3: Softplus 4: ReLU output layer uses logistic transformation (inv_logit) constrain predictions 0 1, aligns Bernoulli likelihood.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_bin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Stan Code for Binary Response Models — generate_stan_code_bin","text":"","code":"# Generate Stan code for a single hidden layer with 10 nodes stan_code <- generate_stan_code_bin(1, c(10)) cat(stan_code) #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes; #>   matrix[n, m] X; #>   array[n] int<lower=0, upper=1> y; #>   int<lower=1, upper=3> act_fn; #> } #>  #> parameters { #>   matrix[m, nodes] w1; #>   vector[nodes] b1; #>   vector[nodes] w_out; #>   real b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes] z1; #>   matrix[n, nodes] a1; #>   vector[n] y_hat; #>  #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn == 1) a1 = tanh(z1); #>   else if (act_fn == 2) a1 = inv_logit(z1); #>   else if (act_fn == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes), z1); #>  #>   y_hat = a1 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   y ~ bernoulli_logit(y_hat); #> }  # Generate Stan code for two hidden layers with 8 and 4 nodes stan_code <- generate_stan_code_bin(2, c(8, 4)) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=0, upper=1> y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   y ~ bernoulli_logit(y_hat); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","title":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"function generates Stan code modeling categorical response using neural networks multiple layers. generated code supports customizable activation functions layer softmax-based prediction categorical output.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"","code":"generate_stan_code_cat(num_layers, nodes)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"num_layers Integer. Number layers neural network. nodes Integer vector. Number nodes layer. length vector must match num_layers, values must positive.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"string containing Stan code specified neural network architecture categorical response model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"Stan code includes following components: Data Block: Defines inputs, response variable, layer configurations, activation functions. Parameters Block: Declares weights biases layers output layer. Transformed Parameters Block: Computes intermediate outputs (z ) layer calculates final predictions (y_hat) using softmax function. Model Block: Specifies priors parameters models categorical response using categorical_logit. Supported activation functions layer: 1: Hyperbolic tangent (tanh) 2: Logistic sigmoid (inv_logit) 3: Softplus (log(1 + exp(x))) Default: Rectified linear unit (ReLU) categorical response (y) assumed take integer values 1 K, K total number categories.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Stan Code for Neural Networks with Categorical Response — generate_stan_code_cat","text":"","code":"# Generate Stan code for a neural network with 3 layers num_layers <- 3 nodes <- c(10, 8, 6) # 10 nodes in the first layer, 8 in the second, 6 in the third stan_code <- generate_stan_code_cat(num_layers, nodes) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   array[n] int<lower=1> y; #>   int<lower=1> act_fn[L]; #>   int<lower=2> K; // Number of categories #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   matrix[nodes[2], nodes[3]] w3; #>   vector[nodes[3]] b3; #>   matrix[nodes[L], K] w_out; #>   vector[K] b_out; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   matrix[n, nodes[3]] z3; #>   matrix[n, nodes[3]] a3; #>   matrix[n, K] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   z3 = a2 * w3 + rep_matrix(b3', n); #>   if (act_fn[3] == 1) a3 = tanh(z3); #>   else if (act_fn[3] == 2) a3 = inv_logit(z3); #>   else if (act_fn[3] == 3) a3 = log(1 + exp(z3)); #>   else a3 = fmax(rep_matrix(0, n, nodes[3]), z3); #>   y_hat = a3 * w_out + rep_matrix(b_out', n); #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   to_vector(w3) ~ PRIOR_WEIGHT; #>   b3 ~ PRIOR_BIAS; #>   to_vector(w_out) ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   for (i in 1:n) y[i] ~ categorical_logit(y_hat[i]'); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","title":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"function generates Stan code Bayesian neural network model designed predict continuous response variables. Stan code dynamically constructed based specified number hidden layers nodes per layer. supports various activation functions hidden layers, including tanh, sigmoid, softplus relu.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"","code":"generate_stan_code_cont(num_layers, nodes)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"num_layers integer specifying number hidden layers neural network. nodes vector integers, element specifies number nodes corresponding hidden layer. length vector must match num_layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"character string containing Stan code specified Bayesian neural network model. Stan model includes data, parameters, transformed parameters, model blocks. code adjusted based whether network one multiple hidden layers.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"generated Stan code models continuous response variable using neural network. hidden layers apply specified activation functions, output layer performs linear transformation predict response. likelihood assumes normally distributed residuals. one hidden layer: function simplifies Stan code structure. multiple hidden layers: code dynamically includes additional layers based input arguments. Supported activation functions hidden layers: 1: Tanh 2: Sigmoid 3: Softplus 4: ReLU","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/generate_stan_code_cont.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Stan Code for Continuous Response Models — generate_stan_code_cont","text":"","code":"# Generate Stan code for a single hidden layer with 10 nodes stan_code <- generate_stan_code_cont(1, c(10)) cat(stan_code) #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes; #>   matrix[n, m] X; #>   vector[n] y; #>   int<lower=1, upper=3> act_fn; #> } #>  #> parameters { #>   matrix[m, nodes] w1; #>   vector[nodes] b1; #>   vector[nodes] w_out; #>   real b_out; #>   real<lower=0> sigma; #> } #>  #> transformed parameters { #>   matrix[n, nodes] z1; #>   matrix[n, nodes] a1; #>   vector[n] y_hat; #>  #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn == 1) a1 = tanh(z1); #>   else if (act_fn == 2) a1 = inv_logit(z1); #>   else if (act_fn == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes), z1); #>  #>   y_hat = a1 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   sigma ~ PRIOR_SIGMA; #>   y ~ normal(y_hat, sigma); #> }  # Generate Stan code for two hidden layers with 8 and 4 nodes stan_code <- generate_stan_code_cont(2, c(8, 4)) cat(stan_code) #>  #> data { #>   int<lower=1> n; #>   int<lower=1> m; #>   int<lower=1> L; #>   int<lower=1> nodes[L]; #>   matrix[n, m] X; #>   vector[n] y; #>   int<lower=1> act_fn[L]; #> } #>  #> parameters { #>   matrix[m, nodes[1]] w1; #>   vector[nodes[1]] b1; #>   matrix[nodes[1], nodes[2]] w2; #>   vector[nodes[2]] b2; #>   vector[nodes[L]] w_out; #>   real b_out; #>   real<lower=0> sigma; #> } #>  #> transformed parameters { #>   matrix[n, nodes[1]] z1; #>   matrix[n, nodes[1]] a1; #>   matrix[n, nodes[2]] z2; #>   matrix[n, nodes[2]] a2; #>   vector[n] y_hat; #>   z1 = X * w1 + rep_matrix(b1', n); #>   if (act_fn[1] == 1) a1 = tanh(z1); #>   else if (act_fn[1] == 2) a1 = inv_logit(z1); #>   else if (act_fn[1] == 3) a1 = log(1 + exp(z1)); #>   else a1 = fmax(rep_matrix(0, n, nodes[1]), z1); #>   z2 = a1 * w2 + rep_matrix(b2', n); #>   if (act_fn[2] == 1) a2 = tanh(z2); #>   else if (act_fn[2] == 2) a2 = inv_logit(z2); #>   else if (act_fn[2] == 3) a2 = log(1 + exp(z2)); #>   else a2 = fmax(rep_matrix(0, n, nodes[2]), z2); #>   y_hat = a2 * w_out + b_out; #> } #>  #> model { #>   to_vector(w1) ~ PRIOR_WEIGHT; #>   b1 ~ PRIOR_BIAS; #>   to_vector(w2) ~ PRIOR_WEIGHT; #>   b2 ~ PRIOR_BIAS; #>   w_out ~ PRIOR_WEIGHT; #>   b_out ~ PRIOR_BIAS; #>   sigma ~ PRIOR_SIGMA; #>   y ~ normal(y_hat, sigma); #> }"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure Performance for Binary Classification Models — measure_bin","title":"Measure Performance for Binary Classification Models — measure_bin","text":"Evaluates performance binary classification model using confusion matrix accuracy.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure Performance for Binary Classification Models — measure_bin","text":"","code":"measure_bin(obs, pred, cut = 0.5)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure Performance for Binary Classification Models — measure_bin","text":"obs numeric integer vector observed binary class labels (0 1). pred numeric vector predicted probabilities positive class. cut numeric threshold (0 1) classify predictions binary labels.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure Performance for Binary Classification Models — measure_bin","text":"list containing: conf_mat confusion matrix comparing observed predicted class labels. accuracy proportion correct predictions. ROC ROC generated using pROC::roc AUC Area ROC curve.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_bin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure Performance for Binary Classification Models — measure_bin","text":"","code":"obs <- c(1, 0, 1, 1, 0) pred <- c(0.9, 0.4, 0.8, 0.7, 0.3) cut <- 0.5 measure_bin(obs, pred, cut) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> $conf_mat #>    pred_label #> obs 0 1 #>   0 2 0 #>   1 0 3 #>  #> $accuracy #> [1] 1 #>  #> $ROC #>  #> Call: #> roc.default(response = obs, predictor = pred) #>  #> Data: pred in 2 controls (obs 0) < 3 cases (obs 1). #> Area under the curve: 1 #>  #> $AUC #> [1] 1 #>  # Returns: list(conf_mat = <confusion matrix>, accuracy = 1, ROC = <ROC>, AUC = 1)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure Performance for Multi-Class Classification Models — measure_cat","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"Evaluates performance multi-class classification model using log loss multiclass AUC.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"","code":"measure_cat(obs, pred)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"obs factor vector observed class labels. level represents unique class. pred numeric matrix predicted probabilities, row corresponds observation, column corresponds class. number columns must match number levels obs.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"list containing: log_loss negative log-likelihood averaged across observations. ROC ROC generated using pROC::roc AUC multiclass Area Curve (AUC) computed pROC::multiclass.roc.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"log loss calculated : $$-\\frac{1}{N} \\sum_{=1}^N \\sum_{c=1}^C y_{ic} \\log(p_{ic})$$ \\(y_{ic}\\) 1 observation \\(\\) belongs class \\(c\\), \\(p_{ic}\\) predicted probability class. AUC computed using pROC::multiclass.roc function, provides overall measure model performance multiclass classification.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure Performance for Multi-Class Classification Models — measure_cat","text":"","code":"library(pROC) #> Type 'citation(\"pROC\")' for a citation. #>  #> Attaching package: ‘pROC’ #> The following objects are masked from ‘package:stats’: #>  #>     cov, smooth, var obs <- factor(c(\"A\", \"B\", \"C\"), levels = LETTERS[1:3]) pred <- matrix(   c(     0.8, 0.1, 0.1,     0.2, 0.6, 0.2,     0.7, 0.2, 0.1   ),   nrow = 3, byrow = TRUE ) measure_cat(obs, pred) #> $log_loss #> [1] 1.012185 #>  #> $ROC #>  #> Call: #> multiclass.roc.default(response = obs, predictor = `colnames<-`(data.frame(pred),     levels(obs))) #>  #> Data: multivariate predictor `colnames<-`(data.frame(pred), levels(obs)) with 3 levels of obs: A, B, C. #> Multi-class area under the curve: 0.75 #>  #> $AUC #> [1] 0.75 #>  # Returns: list(log_loss = 1.012185, ROC = <ROC>, AUC = 0.75)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure Performance for Continuous Response Models — measure_cont","title":"Measure Performance for Continuous Response Models — measure_cont","text":"Evaluates performance continuous response model using RMSE MAE.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure Performance for Continuous Response Models — measure_cont","text":"","code":"measure_cont(obs, pred)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure Performance for Continuous Response Models — measure_cont","text":"obs numeric vector observed (true) values. pred numeric vector predicted values.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Measure Performance for Continuous Response Models — measure_cont","text":"list containing: rmse Root Mean Squared Error. mae Mean Absolute Error.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/measure_cont.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure Performance for Continuous Response Models — measure_cont","text":"","code":"obs <- c(3.2, 4.1, 5.6) pred <- c(3.0, 4.3, 5.5) measure_cont(obs, pred) #> $rmse #> [1] 0.1732051 #>  #> $mae #> [1] 0.1666667 #>  # Returns: list(rmse = 0.1732051, mae = 0.1666667)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for ","title":"Predict Method for ","text":"Generates predictions fitted Bayesian Neural Network (BNN) model.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for ","text":"","code":"# S3 method for class 'bnns' predict(object, newdata = NULL, ...)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for ","text":"object object class \"bnns\", typically result call bnns.default. newdata matrix data frame new input data predictions required. NULL, predictions made training data used fit model. ... Additional arguments (currently used).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for ","text":"matrix/array predicted values(regression)/probabilities(classification) first dimension corresponds rows newdata training data newdata NULL. Second dimension corresponds number posterior samples. case out_act_fn = 3, third dimension corresponds class.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for ","text":"function uses posterior distribution Stan model bnns object compute predictions provided input data.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/predict.bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for ","text":"","code":"# Example usage: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.9e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.005 seconds (Warm-up) #> Chain 1:                0.004 seconds (Sampling) #> Chain 1:                0.009 seconds (Total) #> Chain 1:  new_data <- data.frame(x1 = runif(5), x2 = runif(5)) predictions <- predict(model, newdata = new_data) print(predictions) #>           [,1]      [,2]      [,3]     [,4]      [,5]      [,6]      [,7] #> [1,] 0.8256108 0.6428900 0.7626001 1.560342 0.6564725 1.2863594 0.7081518 #> [2,] 0.5303465 1.0722136 0.7606258 1.570493 0.9851583 0.7255451 0.6667621 #> [3,] 0.5130280 1.0574537 0.7750230 1.594446 0.9898552 0.8672218 0.6508388 #> [4,] 0.5540972 0.8418414 0.8057961 1.650548 0.8564498 1.3516180 0.6269123 #> [5,] 0.7708862 0.6448182 0.7905954 1.615972 0.6192750 1.3800337 0.7051285 #>           [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14] #> [1,] 0.7348802 0.5177157 0.6739890 0.6264617 0.5248645 0.5370976 0.7959615 #> [2,] 0.6844644 0.5801661 1.0124145 0.7814262 0.6489153 0.4659054 0.8495341 #> [3,] 0.6661052 0.6138196 1.1701587 0.8118440 0.6737030 0.5013540 0.8304411 #> [4,] 0.6208066 0.8674174 1.5497628 0.9363655 0.6560523 0.6851629 0.7503133 #> [5,] 0.7131650 0.5460096 0.9025467 0.7433963 0.5364967 0.6487351 0.7279148 #>         [,15]     [,16]     [,17]     [,18]      [,19]     [,20]     [,21] #> [1,] 1.224571 0.2557619 1.1132900 0.4963641 0.05850079 0.8641153 0.1818943 #> [2,] 1.200460 1.5471583 1.1878280 0.8490018 0.41952003 0.8100503 0.6229872 #> [3,] 1.204499 1.5599654 1.0400919 0.8463734 0.39180416 0.8198944 0.6878105 #> [4,] 1.226173 1.0982273 0.6441228 0.7450196 0.16239718 0.8878849 0.5615267 #> [5,] 1.234106 0.3269410 0.9280958 0.4739137 0.02331640 0.8807143 0.2023019 #>            [,22]     [,23]     [,24]     [,25]       [,26]     [,27]     [,28] #> [1,] -0.04955071 0.7397314 0.8924902 0.2548955 -0.02476387 0.2187709 0.7276613 #> [2,]  0.17604580 0.8041785 1.6509867 0.8818389 -0.07332960 1.1616437 0.7088414 #> [3,]  0.19047694 0.8066552 1.6249765 0.9492819 -0.05654353 1.1638078 0.7958651 #> [4,]  0.15297032 0.8013582 0.7979713 0.9868235  0.01433843 1.0728301 1.1522356 #> [5,] -0.13047925 0.7534122 0.4316640 0.5209287  0.01442642 0.4177086 0.9487629 #>          [,29]     [,30]    [,31]     [,32]     [,33]       [,34]     [,35] #> [1,] 0.3721120 0.1815373 1.625360 0.2157526 0.5089144 -0.04351998 0.8345595 #> [2,] 0.7163712 0.4255234 1.592640 0.2362203 0.5371255 -0.10808685 0.7742985 #> [3,] 0.7414811 0.4388964 1.489369 0.2553276 0.5883519 -0.12925993 0.7641736 #> [4,] 0.7778808 0.4353165 1.236109 0.3191370 0.8305072 -0.19620697 0.5855408 #> [5,] 0.4942619 0.1971175 1.368926 0.2592035 0.6305480 -0.08797299 0.8458965 #>          [,36]     [,37]     [,38]     [,39]     [,40]     [,41]      [,42] #> [1,] 0.4318142 0.1026655 0.1680382 0.6788740 0.4417653 0.9916702 0.08230072 #> [2,] 0.3406901 0.2738229 0.2525367 0.3755732 0.6605772 0.9544644 0.59316371 #> [3,] 0.3131725 0.3519238 0.3172546 0.4136415 0.7838046 0.9483716 0.69076586 #> [4,] 0.2594712 0.5638514 0.5831573 0.6465673 1.1035487 0.9392490 0.81617920 #> [5,] 0.3763412 0.2200260 0.2839420 0.7508627 0.7634201 0.9760296 0.26618733 #>         [,43]     [,44]    [,45]     [,46]      [,47]     [,48]     [,49] #> [1,] 1.348254 0.1765521 1.143848 0.4378100 -0.6452489 0.4080875 0.5455994 #> [2,] 1.669433 0.7343292 1.084374 0.5027419  0.3039128 0.1659386 0.6311979 #> [3,] 1.660939 0.8070272 1.118136 0.4841745  0.3072585 0.2651374 0.6557381 #> [4,] 1.447267 0.8584842 1.238541 0.4281814  0.1082506 0.5756661 0.8158539 #> [5,] 1.246141 0.3104976 1.198587 0.4348119 -0.5368357 0.5525906 0.6208059 #>          [,50] #> [1,] 0.5978422 #> [2,] 0.6024875 #> [3,] 0.6131923 #> [4,] 0.6493726 #> [5,] 0.6332901"},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for ","title":"Print Method for ","text":"Displays summary fitted Bayesian Neural Network (BNN) model, including function call Stan fit details.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for ","text":"","code":"# S3 method for class 'bnns' print(x, ...)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for ","text":"x object class \"bnns\", typically result call bnns.default. ... Additional arguments (currently used).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for ","text":"function called side effects return value. prints following: function call used generate \"bnns\" object. summary Stan fit object stored x$fit.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/print.bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Method for ","text":"","code":"# Example usage: data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.006 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:  print(model) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = data, L = 1,  #>     nodes = 2, act_fn = 2, iter = 100, warmup = 50, chains = 1) #>  #> Stan fit: #> Inference for Stan model: anon_model. #> 1 chains, each with iter=100; warmup=50; thin=1;  #> post-warmup draws per chain=50, total post-warmup draws=50. #>  #>            mean se_mean   sd   2.5%    25%    50%   75% 97.5% n_eff Rhat #> w1[1,1]    0.09    0.19 1.21  -1.55  -1.00   0.12  0.87  2.56    40 1.03 #> w1[1,2]   -0.06    0.20 1.39  -2.66  -0.97   0.00  0.96  2.42    50 1.02 #> w1[2,1]   -0.03    0.13 0.85  -2.05  -0.55   0.02  0.46  1.27    41 0.98 #> w1[2,2]   -0.01    0.15 0.93  -1.65  -0.78   0.12  0.66  1.47    40 1.01 #> b1[1]     -0.21    0.16 0.84  -1.94  -0.64  -0.26  0.32  1.38    27 1.14 #> b1[2]     -0.11    0.12 0.82  -1.35  -0.71  -0.17  0.41  1.50    51 0.98 #> w_out[1]   0.08    0.17 1.16  -1.96  -0.72  -0.02  0.97  2.07    49 0.98 #> w_out[2]   0.05    0.19 0.98  -1.59  -0.44  -0.05  0.77  1.78    28 0.99 #> b_out      0.30    0.09 0.74  -1.01  -0.24   0.19  0.90  1.51    63 0.99 #> sigma      1.07    0.03 0.23   0.69   0.95   1.05  1.18  1.56    49 0.98 #> lp__     -11.00    0.35 2.10 -15.17 -12.33 -11.11 -9.55 -7.33    36 0.99 #>  #> Samples were drawn using NUTS(diag_e) at Fri Jan  3 04:39:49 2025. #> For each parameter, n_eff is a crude measure of effective sample size, #> and Rhat is the potential scale reduction factor on split chains (at  #> convergence, Rhat=1)."},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":null,"dir":"Reference","previous_headings":"","what":"relu transformation — relu","title":"relu transformation — relu","text":"relu transformation","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"relu transformation — relu","text":"","code":"relu(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"relu transformation — relu","text":"x numeric vector matrix relu transformation going applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"relu transformation — relu","text":"numeric vector matrix relu transformation.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/relu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"relu transformation — relu","text":"","code":"relu(matrix(1:4, , nrow = 2)) #>      [,1] [,2] #> [1,]    1    3 #> [2,]    2    4"},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"sigmoid transformation — sigmoid","title":"sigmoid transformation — sigmoid","text":"sigmoid transformation","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sigmoid transformation — sigmoid","text":"","code":"sigmoid(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"sigmoid transformation — sigmoid","text":"x numeric vector matrix sigmoid transformation going applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"sigmoid transformation — sigmoid","text":"numeric vector matrix sigmoid transformation.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/sigmoid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"sigmoid transformation — sigmoid","text":"","code":"sigmoid(matrix(1:4, nrow = 2)) #>           [,1]      [,2] #> [1,] 0.7310586 0.9525741 #> [2,] 0.8807971 0.9820138"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Softmax Function to a 3D Array — softmax_3d","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"function applies softmax transformation along third dimension 3D array. softmax function converts raw scores probabilities sum 1 slice along third dimension.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"","code":"softmax_3d(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"x 3D array. input array softmax function applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"3D array dimensions x, values along third dimension transformed using softmax function.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"softmax transformation computed : $$\\text{softmax}(x_{ijk}) = \\frac{\\exp(x_{ijk})}{\\sum_{l} \\exp(x_{ijl})}$$ applied pair indices (, j) across third dimension (k). function processes input array slice--slice first two dimensions (, j), normalizing values along third dimension (k) slice.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softmax_3d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Softmax Function to a 3D Array — softmax_3d","text":"","code":"# Example: Apply softmax to a 3D array x <- array(runif(24), dim = c(2, 3, 4)) # Random 3D array (2x3x4) softmax_result <- softmax_3d(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":null,"dir":"Reference","previous_headings":"","what":"softplus transformation — softplus","title":"softplus transformation — softplus","text":"softplus transformation","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"softplus transformation — softplus","text":"","code":"softplus(x)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"softplus transformation — softplus","text":"x numeric vector matrix softplus transformation going applied.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"softplus transformation — softplus","text":"numeric vector matrix softplus transformation.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/softplus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"softplus transformation — softplus","text":"","code":"softplus(matrix(1:4, nrow = 2)) #>          [,1]     [,2] #> [1,] 1.313262 3.048587 #> [2,] 2.126928 4.018150"},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"Provides comprehensive summary fitted Bayesian Neural Network (BNN) model, including details model call, data, network architecture, posterior distributions, model fitting information.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"","code":"# S3 method for class 'bnns' summary(object, ...)"},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"object object class bnns, representing fitted Bayesian Neural Network model. ... Additional arguments (currently unused).","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"list (returned invisibly) containing following elements: \"Number observations\": number observations training data. \"Number features\": number features training data. \"Number hidden layers\": number hidden layers neural network. \"Nodes per layer\": comma-separated string representing number nodes hidden layer. \"Activation functions\": comma-separated string representing activation functions used hidden layer. \"Output activation function\": activation function used output layer. \"Stanfit Summary\": summary Stan model, including key parameter posterior distributions. \"Iterations\": total number iterations used sampling Bayesian model. \"Warmup\": number iterations used warmup Bayesian model. \"Thinning\": thinning interval used Bayesian model. \"Chains\": number Markov chains used Bayesian model. \"Performance\": Predictive performance metrics, vary based output activation function. function also prints summary console.","code":""},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"function prints following information: Call: original function call used fit model. Data Summary: Number observations features training data. Network Architecture: Structure BNN including number hidden layers, nodes per layer, activation functions. Posterior Summary: Summarized posterior distributions key parameters (e.g., weights, biases, noise parameter). Model Fit Information: Bayesian sampling details, including number iterations, warmup period, thinning, chains. Notes: Remarks warnings, checks convergence diagnostics.","code":""},{"path":[]},{"path":"https://swarnendu-stat.github.io/bnns/reference/summary.bnns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary of a Bayesian Neural Network (BNN) Model — summary.bnns","text":"","code":"# Fit a Bayesian Neural Network data <- data.frame(x1 = runif(10), x2 = runif(10), y = rnorm(10)) model <- bnns(y ~ -1 + x1 + x2,   data = data, L = 1, nodes = 2, act_fn = 2,   iter = 1e2, warmup = 5e1, chains = 1 ) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: WARNING: There aren't enough warmup iterations to fit the #> Chain 1:          three stages of adaptation as currently configured. #> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of #> Chain 1:          the given number of warmup iterations: #> Chain 1:            init_buffer = 7 #> Chain 1:            adapt_window = 38 #> Chain 1:            term_buffer = 5 #> Chain 1:  #> Chain 1: Iteration:  1 / 100 [  1%]  (Warmup) #> Chain 1: Iteration: 10 / 100 [ 10%]  (Warmup) #> Chain 1: Iteration: 20 / 100 [ 20%]  (Warmup) #> Chain 1: Iteration: 30 / 100 [ 30%]  (Warmup) #> Chain 1: Iteration: 40 / 100 [ 40%]  (Warmup) #> Chain 1: Iteration: 50 / 100 [ 50%]  (Warmup) #> Chain 1: Iteration: 51 / 100 [ 51%]  (Sampling) #> Chain 1: Iteration: 60 / 100 [ 60%]  (Sampling) #> Chain 1: Iteration: 70 / 100 [ 70%]  (Sampling) #> Chain 1: Iteration: 80 / 100 [ 80%]  (Sampling) #> Chain 1: Iteration: 90 / 100 [ 90%]  (Sampling) #> Chain 1: Iteration: 100 / 100 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.006 seconds (Warm-up) #> Chain 1:                0.005 seconds (Sampling) #> Chain 1:                0.011 seconds (Total) #> Chain 1:   # Get a summary of the model summary(model) #> Call: #> bnns.default(formula = y ~ -1 + x1 + x2, data = data, L = 1,  #>     nodes = 2, act_fn = 2, iter = 100, warmup = 50, chains = 1) #>  #> Data Summary: #> Number of observations: 10  #> Number of features: 2  #>  #> Network Architecture: #> Number of hidden layers: 1  #> Nodes per layer: 2  #> Activation functions: 2  #> Output activation function: 1  #>  #> Posterior Summary (Key Parameters): #>                 mean    se_mean        sd       2.5%        25%         50% #> w_out[1]  0.03410169 0.09762577 0.8753696 -1.5443422 -0.6013655 -0.09193273 #> w_out[2] -0.07061306 0.05885257 0.5424295 -1.0986872 -0.3915508 -0.02979650 #> b_out     0.02018913 0.07505231 0.5307910 -1.1129881 -0.3353659  0.11058496 #> sigma     0.66135070 0.02251998 0.1435194  0.4300843  0.5612586  0.63920550 #>                75%     97.5%    n_eff      Rhat #> w_out[1] 0.6988798 1.6766091 80.39961 0.9814640 #> w_out[2] 0.2195978 1.0354840 84.94850 0.9798052 #> b_out    0.4626797 0.7465981 50.01714 0.9809754 #> sigma    0.7248632 0.9483875 40.61490 1.0004873 #>  #> Model Fit Information: #> Iterations: 100  #> Warmup: 50  #> Thinning: 1  #> Chains: 1  #>  #> Predictive Performance: #> RMSE (training): 0.540351  #> MAE (training): 0.3847471  #>  #> Notes: #> Check convergence diagnostics for parameters with high R-hat values."},{"path":"https://swarnendu-stat.github.io/bnns/news/index.html","id":"bnns-010","dir":"Changelog","previous_headings":"","what":"bnns 0.1.0","title":"bnns 0.1.0","text":"Initial release bnns package. Implements Bayesian Neural Networks using Stan. Provides flexible model specification formula Bayesian estimation.","code":""}]
